{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1103e8b-8c95-478f-908d-7bdcd58135d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text-to-SQL Fine Tuning Lab\n",
    "\n",
    "**Note** This is a slightly modified version of a notebook from Phil Schmid's blog [here](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)\n",
    "\n",
    "This lab illustrates fine tuning a 7 billion parameter LLM for text to SQL use cases. It's often valuable to include generalized instruction tuning datapoints into the dataset as well as text to sql datapoints to make the model a bit more robust. If you include only sql examples, the model will not generalize as well your users inputs drift over time.\n",
    "\n",
    "## Concepts\n",
    "\n",
    "### Fine tuning:\n",
    "Fine tuning a model is the process taking an already trained model, and further training it on specific tasks. In our case, we'll be training it to follow instructions (using the dolly dataset) as well as a SQL dataset.\n",
    "\n",
    "### LoRA\n",
    "LoRA is a parameter-efficient fine-tuning technique for large language models (LLMs). It works by introducing trainable low-rank decomposition matrices to the weights of the model. Instead of fine-tuning all parameters of a pre-trained model, LoRA freezes the original model weights and injects trainable rank decomposition matrices into each layer of the model.\n",
    "The key idea behind LoRA is to represent the weight updates during fine-tuning as the product of two low-rank matrices. Mathematically, if W is the original weight matrix, the LoRA update can be expressed as:\n",
    "\n",
    "W' = W + BA\n",
    "\n",
    "Where B and A are low-rank matrices, and their product BA represents the update to the original weights.\n",
    "LoRA works effectively for several reasons:\n",
    "\n",
    "* Parameter efficiency: By using low-rank matrices, LoRA dramatically reduces the number of trainable parameters compared to full fine-tuning. This makes it possible to adapt large models on limited hardware.\n",
    "* Preservation of pre-trained knowledge: Since the original weights are kept frozen, the model retains most of its pre-trained knowledge while learning new tasks.\n",
    "Adaptability: The low-rank update allows the model to learn task-specific adaptations without overfitting as easily as full fine-tuning might.\n",
    "* Computational efficiency: Training and applying LoRA updates is computationally cheaper than full fine-tuning or using adapter layers.\n",
    "* Theoretical foundation: The effectiveness of LoRA is grounded in the observation that the weight updates during fine-tuning often have a low intrinsic rank, meaning they can be well-approximated by low-rank matrices.\n",
    "* Composability: Multiple LoRA adaptations can be combined, allowing for interesting multi-task and transfer learning scenarios.\n",
    "\n",
    "The reason LoRA works so well is that it exploits the low intrinsic dimensionality of the updates needed to adapt a pre-trained model to a new task. By focusing on these key directions of change, LoRA can achieve performance comparable to full fine-tuning with only a fraction of the trainable parameters.\n",
    "This approach has proven particularly effective for large language models, where the cost and computational requirements of full fine-tuning can be prohibitive.\n",
    "\n",
    "## Steps\n",
    "1. Install dependencies & setup SageMaker Session\n",
    "2. Create and process our dataset\n",
    "3. Train our model in a notebook\n",
    "\n",
    "# Takeaways\n",
    "There are many ways to fine tune a model. This training job will take roughly ~2 hours on a G5.2xlarge ($1.515 / hr in us-west-2). \n",
    "\n",
    "This means the total training job will cost ~$3.03 dollars. Not bad! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104f597-ab95-4f40-a67c-b5bf1ba6ca05",
   "metadata": {},
   "source": [
    "# 2. Setup development environment\n",
    "Our first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd5db4-6024-42a9-a77e-b804491c896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"torch==2.4.0\" tensorboard\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"transformers==4.44.2\" \\\n",
    "  \"datasets==2.21.0\" \\\n",
    "  \"accelerate==0.33.0\" \\\n",
    "  \"evaluate==0.4.2\" \\\n",
    "  \"bitsandbytes==0.43.3\" \\\n",
    "  \"trl==0.9.6\" \\\n",
    "  \"peft==0.12.0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffcff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HF token\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a57a7-498f-46fa-91aa-36b39aa43ba0",
   "metadata": {},
   "source": [
    "# Install Flash Attention\n",
    "For this lab, flash attention will take too long to install. Leave this commented out. It'll take longer for the model to train without flash attention, so it's recommended to use it when doing this work outside of a lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36cfc4-8981-495e-a845-0a0ebe22a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# # install flash-attn\n",
    "# !pip install ninja packaging\n",
    "# !MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c39d2-4f61-4267-970f-d87b38722094",
   "metadata": {},
   "source": [
    "# Login to Hugging Face\n",
    "We need to log into hugging face to download gated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7de86-175d-455b-aa91-6723d75ef55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbd4c3-c5a8-4a5a-ad6b-f1c67cfab63e",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "To make a more robust model, we're going to take our synthetically generated data and mix it with an instruction dataset + a more generic SQL database. The original instruction tuning paper used ~15k examples, but later research indicates you potentially need way less to get a performant model. \n",
    "\n",
    "Resources: \n",
    "1. [LIMA](https://arxiv.org/abs/2305.11206)\n",
    "2. [Instruct](https://arxiv.org/abs/2203.02155)\n",
    "\n",
    "Most LLMs released to consumers are further refined using reinforcement learning with human feedback. However, you can still get a decent model with regular supervised fine tuning (SFT). In a production system, the dataset would be changing over time and it's not uncommon to have 10s of thousands or even hundreds of thousands of training samples.\n",
    "\n",
    "Because we're training off the base model, the model isn't aligned by default to protect against harmful queries. You should consider further tuning it on alignment data like the dataset provided by Anthropic [here](https://huggingface.co/datasets/Anthropic/hh-rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87233bf7-9d6d-4b25-bb7c-cc6315bb58de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from random import randrange\n",
    "import json\n",
    " \n",
    "# Load dataset from the hub\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sql_dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "\n",
    "# Load our synthetic dataset from disk\n",
    "synthetic_data = []\n",
    "with open('./data/synthetic_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        # Parse each line as a JSON object\n",
    "        synthetic_data.append(json.loads(line.strip()))\n",
    "\n",
    "# Pull it into a huggingface Dataset.\n",
    "synthetic_dataset = Dataset.from_list(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66129bf-1466-428d-93d1-74c5610fdee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "SYSTEM_MESSAGE: str = 'You are a helpful assistant'\n",
    "\n",
    "# Format functions provided by the user, both now returning tuples\n",
    "def format_dolly(sample: Dict[str, str]) -> Tuple[str, str]:\n",
    "    instruction: str = f\"{sample['instruction']}\"\n",
    "    context: str = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else \"\"\n",
    "    \n",
    "    # Join the instruction and context together\n",
    "    user_msg: str = \"\\n\\n\".join([i for i in [instruction, context] if i])\n",
    "    \n",
    "    return user_msg, sample['response']\n",
    "\n",
    "def format_sql(sample: Dict[str, str]) -> Tuple[str, str]:\n",
    "    instruction: str = f\"{sample['question']}\"\n",
    "    context: str = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else \"\"\n",
    "\n",
    "    # Join the instruction and context together\n",
    "    user_msg: str = \"\\n\\n\".join([i for i in [instruction, context] if i])\n",
    "\n",
    "    return user_msg, sample['answer']\n",
    "\n",
    "def format_synthetic_data(sample: Dict[str, str]) -> Tuple[str, str]:\n",
    "    instruction: str = f\"{sample['Question']}\"\n",
    "    context: str = f\"### Context\\n{sample['Context']}\" if len(sample[\"Context\"]) > 0 else \"\"\n",
    "\n",
    "    # Join the instruction and context together\n",
    "    user_msg: str = \"\\n\\n\".join([i for i in [instruction, context] if i])\n",
    "\n",
    "    return user_msg, sample['Query']\n",
    "\n",
    "def create_conversation(sample: Dict[str, str], format_func: callable) -> Dict[str, List[Dict[str, str]]]:\n",
    "    user_msg: str\n",
    "    ai_msg: str\n",
    "    user_msg, ai_msg = format_func(sample)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": ai_msg}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8b40d-32fb-4b2f-b6c6-af933abad947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply formatting and create conversations for each dataset\n",
    "dolly_formatted: Dataset = dolly_dataset.map(\n",
    "    lambda x: create_conversation(x, format_dolly),\n",
    "    remove_columns = dolly_dataset.features,batched=False\n",
    ")\n",
    "sql_formatted: Dataset = sql_dataset.map(\n",
    "    lambda x: create_conversation(x, format_sql),\n",
    "    remove_columns = sql_dataset.features,batched=False\n",
    ")\n",
    "\n",
    "synthetic_formatted: Dataset = synthetic_dataset.map(\n",
    "    lambda x: create_conversation(x, format_synthetic_data),\n",
    "    remove_columns = synthetic_dataset.features,batched=False\n",
    ")\n",
    "\n",
    "# To keep training time down, alternatively you can set the max examples to ~2200 total.\n",
    "dolly_size, sql_size, synthetic_size = 1200, 200, 1000\n",
    "\n",
    "# Balance the datasets\n",
    "balanced_dolly: Dataset = dolly_formatted.shuffle(seed=42).select(range(dolly_size))\n",
    "balanced_sql: Dataset = sql_formatted.shuffle(seed=42).select(range(sql_size))\n",
    "balanced_synthetic: Dataset = synthetic_formatted.shuffle(seed=42).select(range(synthetic_size))\n",
    "\n",
    "# Combine the balanced datasets\n",
    "combined_dataset: Dataset = concatenate_datasets([balanced_dolly, balanced_sql, balanced_synthetic])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "dataset: Dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "# Calculate the number of samples for the test set (10% of total)\n",
    "test_size: int = int(len(dataset) * 0.1)\n",
    "\n",
    "# Split to Test/Train\n",
    "dataset = dataset.train_test_split(test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21419e90-ba97-460e-a774-0faa0d17efa5",
   "metadata": {},
   "source": [
    "# Save dataset to disk\n",
    "Lets save the dataset to our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bdcc9-2a22-4c58-bbb9-60770596fdda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][345][\"messages\"])\n",
    "\n",
    "# save datasets to disk \n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefba60-1460-4f92-b906-d6cd4ea788db",
   "metadata": {},
   "source": [
    "# 4. Fine-tune LLM using trl and the SFTTrainer\n",
    "We are now ready to fine-tune our model. We will use the SFTTrainer from trl to fine-tune our model. The SFTTrainer makes it straightfoward to supervise fine-tune open LLMs. The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including:\n",
    "\n",
    "Dataset formatting, including conversational and instruction format\n",
    "Training on completions only, ignoring prompts\n",
    "Packing datasets for more efficient training\n",
    "PEFT (parameter-efficient fine-tuning) support including Q-LoRA\n",
    "Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\n",
    "We will use the dataset formatting, packing and PEFT features in our example. As peft method we will use QLoRA a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization. If you want to learn more about QLoRA and how it works, check out Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA blog post.\n",
    "\n",
    "Now, lets get started! 🚀\n",
    "\n",
    "First, we need to load our dataset from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1268479-f28f-4f48-8923-4fbce4067e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load jsonl data from disk\n",
    "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a9278-ce11-4329-9d08-1b0f00e3702e",
   "metadata": {},
   "source": [
    "Next, we will load our LLM. For our use case we are going to use Mistral 7B. But we can easily swap out the model for another model, e.g. Llama or Mixtral models, TII Falcon, or any other LLMs by changing our model_id variable. We will use bitsandbytes to quantize our model to 4-bit.\n",
    "\n",
    "Note: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU.\n",
    "\n",
    "Correctly, preparing the LLM and Tokenizer for training chat/conversational models is crucial. We need to add new special tokens to the tokenizer and model and teach to understand the different roles in a conversation. In trl we have a convinient method called setup_chat_format, which:\n",
    "\n",
    "Adds special tokens to the tokenizer, e.g. <|im_start|> and <|im_end|>, to indicate the start and end of a conversation.\n",
    "Resizes the model’s embedding layer to accommodate the new tokens.\n",
    "Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e37fd-fe91-4f3a-9e08-16286bb432a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\" # or `meta-llama/Meta-Llama-3.1-8B`\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\", # Uncomment this line to use flash attention.\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709165f-ff07-40da-8ecb-924ae187d078",
   "metadata": {},
   "source": [
    "The SFTTrainer  supports a native integration with peft, which makes it super easy to efficiently tune LLMs using, e.g. QLoRA. We only need to create our LoraConfig and provide it to the trainer. Our LoraConfig parameters are defined based on the [qlora paper](https://arxiv.org/pdf/2305.14314) and [sebastian's blog post](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5ef5a-6602-4a7a-a912-d29511cba62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed905ff0-ebec-4cf2-8c07-95ff88432b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mistral-7b-text-to-sql\",    # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43324faf-9776-4c19-90bc-591c59f0dd7f",
   "metadata": {},
   "source": [
    "We now have every building block we need to create our SFTTrainer to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e6b9d-30c1-43f2-9823-c1b398066371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 4096 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755aa4ee-26aa-4362-b01e-03376fa8ceea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model \n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aacb3f-0739-4742-a599-ea4f6a94081c",
   "metadata": {},
   "source": [
    "# Congrats!\n",
    "Congrats! You just completed your first training job! In the previous sections, we pulled three datasets together to fine tune a base Mistral 7b model on instructions & SQL generation examples. \n",
    "\n",
    "\n",
    "### Next Steps\n",
    "This training job takes about ~2 hours to run at 3 epochs. You will have your workshop environment for 72 hours. After this workshop you can go back and deploy this model to an endpoint and test it out. It's encoraged that you move to the next lab. We will pull a model trained the same way, deploy it to an endpoint and use that for the rest of the workshop\n",
    "\n",
    "If you'd like to play with the model you trained, you can leave the training job running and follow the appendix steps below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f38ba-9bee-4665-b157-befba8c53616",
   "metadata": {},
   "source": [
    "# Appendix A) Run Inference\n",
    "\n",
    "# Merge LoRA adapter in to the original model\n",
    "When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the merge_and_unload method and then save the model with the save_pretrained method. This will save a default model, which can be used for inference.\n",
    "\n",
    "Note: This requires > 30GB CPU Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8b6e3-c571-4c65-91f9-a97c79bb1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc7689-0afb-4ce3-9694-cd2751a389a1",
   "metadata": {},
   "source": [
    "4. Test Model and run Inference\n",
    "After the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69494d1-26ab-4117-9991-5ce1f10a0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"./code-llama-3-1-8b-text-to-sql\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a412f4-0bb4-43c0-abed-d52bbff74927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "# Test on sample \n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "\n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
