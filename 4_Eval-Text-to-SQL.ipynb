{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Text-to-SQL Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro and Goal\n",
    "This Jupyter Notebook is designed to illustrate Text-to-SQL evaluation.\n",
    "\n",
    "The goal is to take highlight programmatic evaluation metrics as well as LLM as a Judge.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "1. Download ground truth dataset comprised of questions and SQL queries for a given database (e.g. Northwind)\n",
    "2. Evaluate accuracy, cost, and latency of different Text-to-SQL approaches compared to the baseline (zero-shot prompting) for all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Import necessary libraries and load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "os.environ['SQL_DATABASE'] = os.getenv('SQL_DATABASE') # LOCAL, SQLALCHEMY, REDSHIFT\n",
    "os.environ['SQL_DIALECT'] = os.getenv('SQL_DIALECT') # SQlite, PostgreSQL\n",
    "os.environ['ENDPOINT_NAME'] = os.getenv('ENDPOINT_NAME')\n",
    "\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "SQL_DATABASE = os.environ['SQL_DATABASE']\n",
    "SQL_DIALECT = os.environ['SQL_DIALECT']\n",
    "ENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\" #anthropic.claude-3-sonnet-20240229-v1:0\" # anthropic.claude-3-haiku-20240307-v1:0 \"anthropic.claude-3-5-sonnet-20240620-v1:0\" \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "EVAL_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# get ground truth data\n",
    "file_path = './data/ground_truth.jsonl'\n",
    "groundtruth_df = pd.read_json(file_path, lines=True)\n",
    "\n",
    "print(f\"Using database: {SQL_DATABASE} with sql dialect: {SQL_DIALECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chromadb\n",
    "import chromadb\n",
    "import boto3\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize Chroma client from our persisted store\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma\")\n",
    "\n",
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "from utils.chroma import BaseRetrievalTask, ChromaDBRetrievalTask\n",
    "# Define some experiment variables\n",
    "TITAN_TEXT_EMBED_V2_ID: str = 'amazon.titan-embed-text-v2:0'\n",
    "COLLECTION_NAME: str = 'sqlsamples_collection'\n",
    "\n",
    "session = boto3.Session()\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=TITAN_TEXT_EMBED_V2_ID\n",
    ")\n",
    "\n",
    "retrieval_task: BaseRetrievalTask = ChromaDBRetrievalTask(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = COLLECTION_NAME,\n",
    "    embedding_function = embedding_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Grading prompt\n",
    "\n",
    "evaluation_template = \"\"\"You are a SQL expert. \n",
    "                Your task is to evaluate a given SQL query based on a provided SQL schema and question using the criteria provided below.\n",
    " \n",
    "                Evaluation Criteria (Additive Score, 0-5):\n",
    "                1. Context: Award 1 point if the generated SQL query uses only information provided in the SQL schema, without introducing external or fabricated details.\n",
    "                2. Completeness: Add 1 point if the generated SQL query addresses all key elements of the question based on the available SQL schema and Exact Set Match Accuracy (EM) score.\n",
    "                3. ExecutionAccuracy: Add 1 point if the generated SQL query is very close to the groundtruth answer based on Execution Accuracy score.\n",
    "                4. Faultless: Add 1 point if the generated SQL query ran without any errors.\n",
    "                5. ValidEfficiencyScore:  Add 1 point if the runtime of the generated SQL query is similar or better than the the groundtruth qery as measured by the Valid Efficiency Score (VES).\n",
    "                \n",
    "                Evaluation Steps:\n",
    "                1. Read provided context, question and answer carefully.\n",
    "                2. Go through each evaluation criterion one by one and assess whether the answer meets the criteria.\n",
    "                3. Compose your reasoning for each critera, explaining why you did or did not award a point. You can only award full points. \n",
    "                4. Calculate the total score by summing the points awarded.\n",
    "                5. Think through the evaluation criteria inside <thinking></thinking> tags. \n",
    "                Then, output the total score inside <score></score> tags.\n",
    "                Review your formatted response. It needs to be valid XML.\n",
    "    \n",
    "                Original question:\n",
    "                <question>\n",
    "                {question}\n",
    "                </question>\n",
    "\n",
    "                SQL schema:\n",
    "                <sql_schema>\n",
    "                {sql_schema}\n",
    "                </sql_schema>\n",
    "\n",
    "                Generated SQL query based on these instructions:\n",
    "                <sql_query>\n",
    "                {sql_query}\n",
    "                </sql_query>\n",
    "\n",
    "                SQL result based on the generated SQL query:\n",
    "                <sql_query_run_result>\n",
    "                {sql_query_run_result}\n",
    "                </sql_query_run_result>\n",
    "\n",
    "                Any SQL errors that might have occured based on the generated SQL query:\n",
    "                <sql_query_run_error>\n",
    "                {sql_query_run_error}\n",
    "                </sql_query_run_error>\n",
    "\n",
    "                Groundtruth SQL query for comparison with the generated SQL query:\n",
    "                <groundtruth_sql_query>\n",
    "                {groundtruth_sql_query}\n",
    "                </groundtruth_sql_query>\n",
    "                \n",
    "                Execution Accuracy, which compares the generated SQL query to the labeled SQL query to determine if its a match or not: \n",
    "                <ex_score>\n",
    "                {ex_score}\n",
    "                </ex_score>\n",
    "                \n",
    "                Exact Set Match Accuracy (EM), which evaluates if the returned result set actually answers the question, regardless of how the query was written: \n",
    "                <em_score>\n",
    "                {em_score}\n",
    "                </em_score>\n",
    "\n",
    "                Valid Efficiency Score (VES), which compares the runtime of the SQL provided as groundtruth to the generated SQL query:\n",
    "                <ves_score>\n",
    "                {ves_score}\n",
    "                </ves_score>                \n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run eval with zero-shot template \n",
    "# Zero-shot SQL prompt template to establish baseline\n",
    "from utils.eval import AnswerTaskRunner\n",
    "\n",
    "## added \"Always prefix table names with the \"public.\" prefix.\" to support running queries via sqlalchemy \n",
    "zero_shot_sql_template = \"\"\"You are a SQL expert. You will be provided with the original user question and a SQL database schema. \n",
    "                Only return the SQL query and nothing else.\n",
    "                \n",
    "                User question:\n",
    "                <user_question>\n",
    "                {user_question}\n",
    "                </user_question>\n",
    "\n",
    "                SQL database schema:\n",
    "                <sql_database_schema>\n",
    "                {sql_database_schema}\n",
    "                </sql_database_schema>\n",
    "                \n",
    "                SQL dialect:\n",
    "                <sql_dialect>\n",
    "                {sql_dialect}\n",
    "                </sql_dialect>\n",
    "\n",
    "                Instructions:\n",
    "                Generate a SQL query that answers the original user question.\n",
    "                Use the schema, first create a syntactically correct {sql_dialect} query to answer the question. \n",
    "                Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
    "                Always prefix table names with the \"public.\" prefix.\n",
    "                Pay attention to use only the column names that you can see in the schema description. \n",
    "                Be careful to not query for columns that do not exist. \n",
    "                Pay attention to which column is in which table. \n",
    "                Also, qualify column names with the table name when needed.\n",
    "                If you cannot answer the user question with the help of the provided SQL database schema, \n",
    "                then output that this question question cannot be answered based of the information stored in the database.\n",
    "                You are required to use the following format, each taking one line:\n",
    "                Return the sql query inside the <sql></sql> tab.\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "answer_results1: pd.DataFrame = AnswerTaskRunner(groundtruth_df[:10],\n",
    "                 model_id=MODEL_ID,\n",
    "                 eval_model_id=EVAL_MODEL_ID,\n",
    "                 sql_database=SQL_DATABASE,\n",
    "                 sql_dialect=SQL_DIALECT,\n",
    "                 prompt_template=zero_shot_sql_template,\n",
    "                 prompt_eval_template=evaluation_template).run()\n",
    "\n",
    "answer_results1.to_json('./data/zero-shot-graded.jsonl', orient='records', lines=True, force_ascii=False, date_format='iso', default_handler=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_results1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Run eval with few-shot template\n",
    "\n",
    "few_shot_sql_template = \"\"\"You are a SQL expert. You will be provided with the original user question and a SQL database schema. \n",
    "                Only return the SQL query and nothing else.\n",
    "                Here is the original user question.\n",
    "                <user_question>\n",
    "                {user_question}\n",
    "                </user_question>\n",
    "\n",
    "                Here is the SQL database schema.\n",
    "                <sql_database_schema>\n",
    "                {sql_database_schema}\n",
    "                </sql_database_schema>\n",
    "\n",
    "                Here are some examples of SQL queries that answer similar questions:\n",
    "                <sql_examples>\n",
    "                {sql_examples}\n",
    "                </sql_examples>\n",
    "                \n",
    "                Instructions:\n",
    "                Generate a SQL query that answers the original user question.\n",
    "                Use the schema, first create a syntactically correct {sql_dialect} query to answer the question. \n",
    "                Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
    "                Always prefix table names with the \"public.\" prefix.\n",
    "                Pay attention to use only the column names that you can see in the schema description. \n",
    "                Be careful to not query for columns that do not exist. \n",
    "                Pay attention to which column is in which table. \n",
    "                Also, qualify column names with the table name when needed.\n",
    "                If you cannot answer the user question with the help of the provided SQL database schema, \n",
    "                then output that this question question cannot be answered based of the information stored in the database.\n",
    "                You are required to use the following format, each taking one line.\n",
    "                Return the sql query inside the <SQL></SQL> tab.\n",
    "                \"\"\"\n",
    "answer_results2: pd.DataFrame = AnswerTaskRunner(groundtruth_df,\n",
    "                 model_id=MODEL_ID,\n",
    "                 eval_model_id=EVAL_MODEL_ID,\n",
    "                 sql_database=SQL_DATABASE,\n",
    "                 sql_dialect=SQL_DIALECT,\n",
    "                 prompt_template=few_shot_sql_template,\n",
    "                 prompt_eval_template=evaluation_template\n",
    "                 retrieval_task=retrieval_task).run()\n",
    "\n",
    "answer_results2.to_json('./data/few-shot-graded.jsonl', orient='records', lines=True, force_ascii=False, date_format='iso', default_handler=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Compare results\n",
    "from utils.util import Util\n",
    "util = Util()\n",
    "util.compare_results(answer_results2, answer_results1)\n",
    "print('zero-shot score distribution')\n",
    "util.visualize_distribution(answer_results1, key='score')\n",
    "print('few-shot score distribution')\n",
    "util.visualize_distribution(answer_results2,key='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Review results\n",
    "print(f'sum of zero_shot_cost: {answer_results1[\"cost\"].sum()}')\n",
    "print(f'sum of few_shot_cost: {answer_results2[\"cost\"].sum()}')\n",
    "print(f'avg of zero_shot_query_time: {answer_results1[\"latency\"].mean()}')\n",
    "print(f'avg of few_shot_query_time: {answer_results2[\"latency\"].mean()}')\n",
    "\n",
    "# Execution Accuracy, which compares the generated SQL query to the ground truth SQL query to determine if its a match\n",
    "print(f'avg of ex_score of zero_shot: {answer_results1[\"ex_score\"].mean()}')\n",
    "print(f'avg of ex_score of few_shot: {answer_results2[\"ex_score\"].mean()}')\n",
    "# Exact Set Match Accuracy (EM), which evaluates if the generated SQL query resultset matches the ground truth resultset\n",
    "print(f'avg of em_score of zero_shot: {answer_results1[\"em_score\"].mean()}')\n",
    "print(f'avg of em_score of few_shot: {answer_results2[\"em_score\"].mean()}')\n",
    "# Valid Efficiency Score (VES), which compares the  generated router SQL query runtime provided to the ground truth SQL query runtime\n",
    "print(f'avg of ves_score of zero_shot: {answer_results1[\"ves_score\"].mean()}')\n",
    "print(f'avg of ves_score of few_shot: {answer_results2[\"ves_score\"].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7. Run eval with finetuned LLM and zero-shot template\n",
    "from utils.eval import AnswerTaskRunner\n",
    "\n",
    "finetuned_sql_template = \"\"\"{user_question} \\n\\n### Context {sql_database_schema}\"\"\"\n",
    "\n",
    "answer_results3: pd.DataFrame = AnswerTaskRunner(groundtruth_df[:10],\n",
    "                 model_id=\"\",\n",
    "                 endpoint_name=ENDPOINT_NAME,\n",
    "                 max_token_count=512,\n",
    "                 eval_model_id=EVAL_MODEL_ID,\n",
    "                 sql_database=SQL_DATABASE,\n",
    "                 sql_dialect=SQL_DIALECT,\n",
    "                 prompt_template=finetuned_sql_template,\n",
    "                 prompt_eval_template=evaluation_template,\n",
    "                 region=REGION).run()\n",
    "\n",
    "answer_results3.to_json('./data/zero-shot-finetuned-graded.jsonl', orient='records', lines=True, force_ascii=False, date_format='iso', default_handler=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_results3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Compare fine-tuned model results with zero-shot prompting\n",
    "\n",
    "print(f'avg of zero-shot_query_time: {answer_results1[\"latency\"].mean()}')\n",
    "print(f'avg of finetuned_query_time: {answer_results3[\"latency\"].mean()}')\n",
    "\n",
    "# Execution Accuracy, which compares the generated SQL query to the ground truth SQL query to determine if its a match\n",
    "print(f'avg of ex_score of zero_shot: {answer_results1[\"ex_score\"].mean()}')\n",
    "print(f'avg of ex_score of finetuned: {answer_results3[\"ex_score\"].mean()}')\n",
    "# Exact Set Match Accuracy (EM), which evaluates if the generated SQL query resultset matches the ground truth resultset\n",
    "print(f'avg of em_score of zero_shot: {answer_results1[\"em_score\"].mean()}')\n",
    "print(f'avg of em_score of finetuned: {answer_results3[\"em_score\"].mean()}')\n",
    "# Valid Efficiency Score (VES), which compares the  generated router SQL query runtime provided to the ground truth SQL query runtime\n",
    "print(f'avg of ves_score of zero_shot: {answer_results1[\"ves_score\"].mean()}')\n",
    "print(f'avg of ves_score of finetuned: {answer_results3[\"ves_score\"].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Util()\n",
    "metrics = ['score', 'ex_score', 'em_score', 'ves_score'] # 'latency' ,'cost'\n",
    "\n",
    "print('zero-shot score distribution')\n",
    "util.visualize_distribution(answer_results1, key='score')\n",
    "print('finetuned score distribution')\n",
    "util.visualize_distribution(answer_results3, key='score')\n",
    "util.compare_results(answer_results1, answer_results3, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "XXX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
