{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f4040b",
   "metadata": {},
   "source": [
    "# Text-to-SQL Fine Tuning Lab\n",
    "\n",
    "**Note** This requires a G5.2xlarge for the SageMaker training job. Most AWS lab environments don't have permissions to use this instance for a training job so it's recommended to run 3(a) within a SageMaker notebook (different than SM studio) running on a G5.2xlarge\n",
    "\n",
    "This lab illustrates fine tuning a 7 billion parameter LLM for text to SQL use cases. It's often valuable to include generalized instruction tuning datapoints into the dataset as well as text to sql datapoints to make the model a bit more robust. If you include only sql examples, the model will not generalize as well your users inputs drift over time.\n",
    "\n",
    "## Concepts\n",
    "\n",
    "### Fine tuning:\n",
    "Fine tuning a model is the process taking an already trained model, and further training it on specific tasks. In our case, we'll be training it to follow instructions (using the dolly dataset) as well as a SQL dataset.\n",
    "\n",
    "### LoRA\n",
    "LoRA is a parameter-efficient fine-tuning technique for large language models (LLMs). It works by introducing trainable low-rank decomposition matrices to the weights of the model. Instead of fine-tuning all parameters of a pre-trained model, LoRA freezes the original model weights and injects trainable rank decomposition matrices into each layer of the model.\n",
    "The key idea behind LoRA is to represent the weight updates during fine-tuning as the product of two low-rank matrices. Mathematically, if W is the original weight matrix, the LoRA update can be expressed as:\n",
    "\n",
    "W' = W + BA\n",
    "\n",
    "Where B and A are low-rank matrices, and their product BA represents the update to the original weights.\n",
    "LoRA works effectively for several reasons:\n",
    "\n",
    "* Parameter efficiency: By using low-rank matrices, LoRA dramatically reduces the number of trainable parameters compared to full fine-tuning. This makes it possible to adapt large models on limited hardware.\n",
    "* Preservation of pre-trained knowledge: Since the original weights are kept frozen, the model retains most of its pre-trained knowledge while learning new tasks.\n",
    "Adaptability: The low-rank update allows the model to learn task-specific adaptations without overfitting as easily as full fine-tuning might.\n",
    "* Computational efficiency: Training and applying LoRA updates is computationally cheaper than full fine-tuning or using adapter layers.\n",
    "* Theoretical foundation: The effectiveness of LoRA is grounded in the observation that the weight updates during fine-tuning often have a low intrinsic rank, meaning they can be well-approximated by low-rank matrices.\n",
    "* Composability: Multiple LoRA adaptations can be combined, allowing for interesting multi-task and transfer learning scenarios.\n",
    "\n",
    "The reason LoRA works so well is that it exploits the low intrinsic dimensionality of the updates needed to adapt a pre-trained model to a new task. By focusing on these key directions of change, LoRA can achieve performance comparable to full fine-tuning with only a fraction of the trainable parameters.\n",
    "This approach has proven particularly effective for large language models, where the cost and computational requirements of full fine-tuning can be prohibitive.\n",
    "\n",
    "## Steps\n",
    "1. Install dependencies & setup SageMaker Session\n",
    "2. Create and process our dataset\n",
    "3. Configure our SageMaker training job\n",
    "4. Run training job\n",
    "\n",
    "# Takeaways\n",
    "There are many ways to fine tune a model. This training job will take roughly ~6 hours on a G5.2xlarge ($1.515 / hr in us-west-2). \n",
    "\n",
    "This means the total training job will cost ~$9.09 dollars. Not bad! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dede76-a9fa-46f6-be5b-1c126ea4ea78",
   "metadata": {},
   "source": [
    "# 1. Setup development environment\n",
    "Our first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16995a32-a9e2-4c5a-a325-dc3c9a7fc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d24aff-c70f-4943-9def-a28fea1aabb0",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "To make a more robust model, we're going to take our synthetically generated data and mix it with an instruction dataset + a more generic SQL database. The original instruction tuning paper used ~15k examples, but later research indicates you potentially need way less to get a performant model. \n",
    "\n",
    "Resources: \n",
    "1. [LIMA](https://arxiv.org/abs/2305.11206)\n",
    "2. [Instruct](https://arxiv.org/abs/2203.02155)\n",
    "\n",
    "Most LLMs released to consumers are further refined using reinforcement learning with human feedback. However, you can still get a decent model with regular supervised fine tuning (SFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e72b1-ad1e-456c-af0e-00aa3b405644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from random import randrange\n",
    "import json\n",
    " \n",
    "# Load dataset from the hub\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sql_dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "\n",
    "# Load our synthetic dataset from disk\n",
    "synthetic_data = []\n",
    "with open('./data/synthetic_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        # Parse each line as a JSON object\n",
    "        synthetic_data.append(json.loads(line.strip()))\n",
    "\n",
    "# Pull it into a huggingface Dataset.\n",
    "synthetic_dataset = Dataset.from_list(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ef949-bc58-405e-8232-baf0f715c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "SYSTEM_MESSAGE: str = 'You are a helpful assistant'\n",
    "\n",
    "# Format functions provided by the user, both now returning tuples\n",
    "def format_dolly(sample: Dict[str, str]) -> Tuple[str, str]:\n",
    "    instruction: str = f\"{sample['instruction']}\"\n",
    "    context: str = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else \"\"\n",
    "    \n",
    "    # Join the instruction and context together\n",
    "    user_msg: str = \"\\n\\n\".join([i for i in [instruction, context] if i])\n",
    "    \n",
    "    return user_msg, sample['response']\n",
    "\n",
    "def format_sql(sample: Dict[str, str]) -> Tuple[str, str]:\n",
    "    instruction: str = f\"{sample['question']}\"\n",
    "    context: str = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else \"\"\n",
    "    \n",
    "    # Join the instruction and context together\n",
    "    user_msg: str = \"\\n\\n\".join([i for i in [instruction, context] if i])\n",
    "    \n",
    "    return user_msg, sample['answer']\n",
    "\n",
    "def format_synthetic_data(sample: Dict[str, str]) -> Tuple[str, str]:\n",
    "    instruction: str = f\"{sample['Question']}\"\n",
    "    context: str = f\"### Context\\n{sample['Context']}\" if len(sample[\"Context\"]) > 0 else \"\"\n",
    "    \n",
    "    # Join the instruction and context together\n",
    "    user_msg: str = \"\\n\\n\".join([i for i in [instruction, context] if i])\n",
    "    \n",
    "    return user_msg, sample['Query']\n",
    "\n",
    "def create_conversation(sample: Dict[str, str], format_func: callable) -> Dict[str, List[Dict[str, str]]]:\n",
    "    user_msg: str\n",
    "    ai_msg: str\n",
    "    user_msg, ai_msg = format_func(sample)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": ai_msg}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d94d0-14c8-410c-b4d6-323abaf298e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply formatting and create conversations for each dataset\n",
    "dolly_formatted: Dataset = dolly_dataset.map(\n",
    "    lambda x: create_conversation(x, format_dolly),\n",
    "    remove_columns = dolly_dataset.features,batched=False\n",
    ")\n",
    "sql_formatted: Dataset = sql_dataset.map(\n",
    "    lambda x: create_conversation(x, format_sql),\n",
    "    remove_columns = sql_dataset.features,batched=False\n",
    ")\n",
    "\n",
    "synthetic_formatted: Dataset = synthetic_dataset.map(\n",
    "    lambda x: create_conversation(x, format_synthetic_data),\n",
    "    remove_columns = synthetic_dataset.features,batched=False\n",
    ")\n",
    "\n",
    "# To keep training time down, alternatively you can set the max examples to ~1200 total.\n",
    "dolly_size, sql_size, synthetic_size = 1200, 200, 1000\n",
    "\n",
    "# Balance the datasets\n",
    "balanced_dolly: Dataset = dolly_formatted.shuffle(seed=42).select(range(dolly_size))\n",
    "balanced_sql: Dataset = sql_formatted.shuffle(seed=42).select(range(sql_size))\n",
    "balanced_synthetic: Dataset = synthetic_formatted.shuffle(seed=42).select(range(synthetic_size))\n",
    "\n",
    "# Combine the balanced datasets\n",
    "combined_dataset: Dataset = concatenate_datasets([balanced_dolly, balanced_sql, balanced_synthetic])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "dataset: Dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "# Calculate the number of samples for the test set (10% of total)\n",
    "test_size: int = int(len(dataset) * 0.1)\n",
    "\n",
    "# Split to Test/Train\n",
    "dataset = dataset.train_test_split(test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc61516-ceb8-4db3-be67-c828f1ed048b",
   "metadata": {},
   "source": [
    "# Save Dataset\n",
    "We'll use the S3 extension of datasets to save this dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd70fe-1e9f-48e0-a78e-00e81fb838f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save datasets to disk if you'd like\n",
    "# dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "# dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c41c3-cad9-422e-ac04-623ac7dd8729",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/text-to-sql-instruct'\n",
    "\n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc716f36-b215-4d4c-9e52-0c88df73ba16",
   "metadata": {},
   "source": [
    "# Log in to Hugging Face\n",
    "Mistral 7b is gated in hugging face. To continue you'll need your hugging face token which should have been added to the dev.env file during the start of the lab. \n",
    "\n",
    "Additionally, make sure you've requested access to the model here: https://huggingface.co/mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c201e-27ae-4778-bbd9-ba23b402450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedaec8b-2fa4-42bc-ba3e-506a64c77116",
   "metadata": {},
   "source": [
    "# Fine-tune LLM using trl on Amazon SageMaker\n",
    "We are now ready to fine-tune our model. We will use the SFTTrainer from trl to fine-tune our model. The SFTTrainer makes it straightfoward to supervise fine-tune open LLMs. The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b27b96-4fd1-4fd1-9f4f-614b2f6faab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/train_dataset.json', # path where sagemaker will save training dataset\n",
    "  'model_id': \"mistralai/Mistral-7B-v0.1\",           # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 4096,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 3,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch_fused\",                      # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'learning_rate': 2e-4,                             # learning rate, based on QLoRA paper\n",
    "  'bf16': True,                                      # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7deb0-e561-4839-a29c-f4fdc8dbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'mistral7b-text-to-sql'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'trl_sft.py',      # train script\n",
    "    source_dir           = './scripts',       # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge',   #'ml.g5.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    environment          = {\n",
    "                            \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "                            \"HF_TOKEN\": HF_TOKEN, # huggingface token to access gated models, e.g. llama 2\n",
    "                            # \"DISABLE_FLASH_ATTENTION\": \"1\" # disable flash attention\n",
    "                            }, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824ef1e-6893-4e58-a5d2-f9550af5b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66c23e-c8a0-4fbe-be64-4e7b278b9589",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the current job name\n",
    "print(huggingface_estimator._current_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124d8a2-91c4-45e5-ac1e-6d634efd3973",
   "metadata": {},
   "source": [
    "# Congrats!\n",
    "Congrats! You just kicked off your first training job! In the previous sections, we pulled two datasets together to fine tune a base Mistral 7b model on instructions & SQL generation examples. \n",
    "\n",
    "\n",
    "### Next Steps\n",
    "This training job takes about ~1 hour to run at 3 epochs. You will have your workshop environment for 72 hours. After this workshop you can go back and deploy this model to an endpoint and test it out. It's encoraged that you move to the next lab. We will pull a LoRA adapter trained using the same script, merge it into the same base model model and use that for the rest of the workshop\n",
    "\n",
    "If you'd like to play with the model you trained, you can leave the training job running and follow the appendix steps below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b61791-a3f8-4ea4-a48f-e99dfba08d09",
   "metadata": {},
   "source": [
    "# Appendix A) Try out the model\n",
    "Once the training job is completed, you can use this code to create a SageMaker endpoint and test out the model you trained in this notebook. The loRA adapter you trained in the previous step and the one we pull in the next lab will operate very similarly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3e5c6-8f8a-4b4d-9c73-7d62609943f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "TRAINING_JOB_NAME = huggingface_estimator._current_job_name\n",
    "\n",
    "huggingface_estimator = Estimator.attach(TRAINING_JOB_NAME, sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3de8cd-b73b-41be-9e01-0b0b3427e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"].replace(\"s3://\", \"https://s3.console.aws.amazon.com/s3/buckets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf3621-04c7-45c2-8895-c9ac31e82995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.0\",\n",
    "  session=sess,\n",
    ")\n",
    " \n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c386f1-03da-4837-82d0-fa7fad77f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# s3 path where the model will be uploaded\n",
    "# if you try to deploy the model to a different time add the s3 path here\n",
    "model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    " \n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  model_data={'S3DataSource':{'S3Uri': model_s3_path,'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f9b72-c78b-4fdb-b7c7-ce18c330d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dbccd-14fa-4919-82ae-ea6a66241452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    " \n",
    "# Load the test dataset from s3\n",
    "S3Downloader.download(f\"{training_input_path}/test_dataset.json\", \".\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\",split=\"train\")\n",
    "random_sample = test_dataset[200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6dea9c-15c6-4f0f-b164-03f868dea6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the tokenizer so that we can use the apply_chat_template() function. This is only on the instruct version of the tokenizer.\n",
    "# We essentially recreated this function above when formatting our inputs.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "random_sample = test_dataset[20]\n",
    "\n",
    "def request(sample):\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = llm.predict({\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": [\"<|im_end|>\"],\n",
    "      }\n",
    "\n",
    "    })\n",
    "    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n",
    "\n",
    "\n",
    "print(random_sample[\"messages\"])\n",
    "\n",
    "# We don't need the answer to do inference.\n",
    "request(random_sample[\"messages\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c256b-17ef-4d12-8297-834f8a5042c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
