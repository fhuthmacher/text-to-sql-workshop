{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Fine Tuned Model\n",
    "This lab builds off the previous model training lab and will deploy the model we trained. Because the training job takes ~6 hours, we've uploaded a model trained using the same script to Hugging Face's model hub so that we can pull it into our inference container and create a sagemaker endpoint. \n",
    "\n",
    "## Steps\n",
    "1. Install dependencies and create our sagemaker session\n",
    "2. Load our model into a Hugging Face inference container (based on TGI)\n",
    "3. Do a \"vibe check\" to make sure the model can generate SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::732004207293:role/SageMakerExecutionRole_029eb6589aed\n",
      "sagemaker bucket: sagemaker-us-west-2-732004207293\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import dotenv\n",
    "\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "\n",
    "os.environ['ENDPOINT_NAME'] = os.getenv('ENDPOINT_NAME')\n",
    "ENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"tannermcrae/Mistral-7B-v0.1-Text2SQL-Instruct\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 'ENDPOINT_NAME',\n",
       " 'huggingface-pytorch-tgi-inference-2024-10-01-18-29-59-268')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f'Endpoint name is: {llm._get_model_names()[0]}')\n",
    "\n",
    "# ENDPOINT_NAME = llm._get_model_names()[0]\n",
    "ENDPOINT_NAME = \"huggingface-pytorch-tgi-inference-2024-10-01-18-29-59-268\"\n",
    "os.environ['ENDPOINT_NAME'] = ENDPOINT_NAME\n",
    "dotenv.set_key(local_env_filename, \"ENDPOINT_NAME\", os.environ[\"ENDPOINT_NAME\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Vibe Check - Text to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.11/site-packages (0.15.2)\n",
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.2\n",
      "    Uninstalling transformers-4.38.2:\n",
      "      Successfully uninstalled transformers-4.38.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.1.1 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "autogluon-multimodal 1.1.1 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires transformers[sentencepiece]<4.41.0,>=4.38.0, but you have transformers 4.45.1 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires gluonts==0.15.1, but you have gluonts 0.14.3 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires transformers[sentencepiece]<4.41.0,>=4.38.0, but you have transformers 4.45.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.20.0 transformers-4.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "def call_sagemaker_endpoint(endpoint_name, sample):\n",
    "    # Create a SageMaker runtime client\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "    # Use tokenizers chat template to format the incomming request\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Add hyperparams and inputs into sagemaker call.\n",
    "    input_data = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"do_sample\": False,\n",
    "            \"return_full_text\": False,\n",
    "            \"stop\": [\"<|im_end|>\"],\n",
    "          }\n",
    "    }\n",
    "\n",
    "    # Convert input data to JSON string\n",
    "    input_json = json.dumps(input_data)\n",
    "\n",
    "    try:\n",
    "        # Call the SageMaker endpoint\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=input_json\n",
    "        )\n",
    "\n",
    "        # Get the response body and decode it\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        \n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling SageMaker endpoint: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = {\n",
    "    'messages': [\n",
    "        {\n",
    "            'content': 'You are an AI assistant that generates SQL queries from natural language and given schema information. Create accurate SQL queries based on the user\\'s request and the provided table structures.',\n",
    "            'role': 'system'\n",
    "        },\n",
    "        {\n",
    "            'content': \"How many books has each author published? List the author names and book counts, but only for authors who have published more than 5 books. Order the results by the number of books in descending order.\\n\\n### Context\\nCREATE TABLE authors (author_id INT PRIMARY KEY, author_name VARCHAR(100));\\nCREATE TABLE books (book_id INT PRIMARY KEY, title VARCHAR(255), author_id INT);\",\n",
    "            'role': 'user'\n",
    "        },\n",
    "        {\n",
    "            'content': \"SELECT a.author_name, COUNT(b.book_id) AS book_count\\nFROM authors a\\nJOIN books b ON a.author_id = b.author_id\\nGROUP BY a.author_id, a.author_name\\nHAVING COUNT(b.book_id) > 5\\nORDER BY book_count DESC;\",\n",
    "            'role': 'assistant'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = call_sagemaker_endpoint(ENDPOINT_NAME, sample_input['messages'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Response:\n",
      "SELECT a.author_name, COUNT(b.book_id) AS book_count\n",
      "FROM authors a\n",
      "JOIN books b ON a.author_id = b.author_id\n",
      "GROUP BY a.author_id, a.author_name\n",
      "HAVING COUNT(b.book_id) > 5\n",
      "ORDER BY book_count DESC;\n",
      "\n",
      "*******\n",
      "\n",
      "\n",
      "Model Response:\n",
      " SELECT author_name, COUNT(*) FROM authors AS a JOIN books AS b ON a.author_id = b.author_id GROUP BY a.author_name HAVING COUNT(*) > 5 ORDER BY COUNT(*) DESC \n"
     ]
    }
   ],
   "source": [
    "print(f\"Correct Response:\\n{sample_input['messages'][2]['content']}\\n\\n*******\\n\\n\")\n",
    "print(f\"Model Response:\\n{response[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "In this lab, we took the model we trained, packaged it up into an inference container, and deployed it to SageMaker. We then validated we could get valid SQL and answer basic instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
