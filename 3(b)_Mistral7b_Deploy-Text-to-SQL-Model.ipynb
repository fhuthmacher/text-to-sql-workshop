{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Fine Tuned Model\n",
    "This lab builds off the previous model training lab and will deploy the model we trained. Because the training job takes ~6 hours, we've uploaded a model trained using the same script to Hugging Face's model hub so that we can pull it into our inference container and create a sagemaker endpoint. \n",
    "\n",
    "## Steps\n",
    "1. Install dependencies and create our sagemaker session\n",
    "2. Load our model into a Hugging Face inference container (based on TGI)\n",
    "3. Do a \"vibe check\" to make sure the model can generate SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import dotenv\n",
    "\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "\n",
    "os.environ['ENDPOINT_NAME'] = os.getenv('ENDPOINT_NAME')\n",
    "ENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"tannermcrae/Mistral-7B-v0.1-Text2SQL-Instruct\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = llm.endpoint\n",
    "\n",
    "print(f'Endpoint name is: {ENDPOINT_NAME}')\n",
    "os.environ['ENDPOINT_NAME'] = ENDPOINT_NAME\n",
    "dotenv.set_key(local_env_filename, \"ENDPOINT_NAME\", os.environ[\"ENDPOINT_NAME\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Vibe Check - Text to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure we have the latest transformers and tokenizers version\n",
    "!pip install --upgrade transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "def call_sagemaker_endpoint(endpoint_name, sample):\n",
    "    # Create a SageMaker runtime client\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "    # Use tokenizers chat template to format the incomming request\n",
    "    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Add hyperparams and inputs into sagemaker call.\n",
    "    input_data = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"do_sample\": False,\n",
    "            \"return_full_text\": False,\n",
    "            \"stop\": [\"<|im_end|>\"],\n",
    "          }\n",
    "    }\n",
    "\n",
    "    # Convert input data to JSON string\n",
    "    input_json = json.dumps(input_data)\n",
    "\n",
    "    try:\n",
    "        # Call the SageMaker endpoint\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=input_json\n",
    "        )\n",
    "\n",
    "        # Get the response body and decode it\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        \n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling SageMaker endpoint: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = {\n",
    "    'messages': [\n",
    "        {\n",
    "            'content': 'You are an AI assistant that generates SQL queries from natural language and given schema information. Create accurate SQL queries based on the user\\'s request and the provided table structures.',\n",
    "            'role': 'system'\n",
    "        },\n",
    "        {\n",
    "            'content': \"How many books has each author published? List the author names and book counts, but only for authors who have published more than 5 books. Order the results by the number of books in descending order.\\n\\n### Context\\nCREATE TABLE authors (author_id INT PRIMARY KEY, author_name VARCHAR(100));\\nCREATE TABLE books (book_id INT PRIMARY KEY, title VARCHAR(255), author_id INT);\",\n",
    "            'role': 'user'\n",
    "        },\n",
    "        {\n",
    "            'content': \"SELECT a.author_name, COUNT(b.book_id) AS book_count\\nFROM authors a\\nJOIN books b ON a.author_id = b.author_id\\nGROUP BY a.author_id, a.author_name\\nHAVING COUNT(b.book_id) > 5\\nORDER BY book_count DESC;\",\n",
    "            'role': 'assistant'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = call_sagemaker_endpoint(ENDPOINT_NAME, sample_input['messages'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Correct Response:\\n{sample_input['messages'][2]['content']}\\n\\n*******\\n\\n\")\n",
    "print(f\"Model Response:\\n{response[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "In this lab, we took the model we trained, packaged it up into an inference container, and deployed it to SageMaker. We then validated we could get valid SQL and answer basic instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
