{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Text-to-SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro and Goal\n",
    "This Jupyter Notebook is designed to illustrate a zero-shot Text-to-SQL approach on the Northwind database.\n",
    "\n",
    "The goal is to take a user prompt along with a SQL database schema, and then generate a corresponding SQL query.\n",
    "\n",
    "### Steps\n",
    "1. Download SQL schema\n",
    "2. Download ground truth dataset comprised of questions and SQL queries for a our sample database (e.g. Northwind)\n",
    "3. Generate and run SQL queries with a smaller LLM\n",
    "4. Generate and run SQL queries with a larger LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a python environment\n",
    "\n",
    "# !conda create -y --name bedrock-router-eval python=3.11.8\n",
    "# !conda init && activate bedrock-router-eval\n",
    "# !conda install -n bedrock-router-eval ipykernel --update-deps --force-reinstall -y\n",
    "# !conda install -c conda-forge ipython-sql\n",
    "\n",
    "## OR\n",
    "# !python -m venv venv\n",
    "# !source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.35.27)\n",
      "Requirement already satisfied: scipy in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.14.1)\n",
      "Requirement already satisfied: numpy in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (3.9.2)\n",
      "Requirement already satisfied: pandas in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: importlib_resources==6.4.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (6.4.0)\n",
      "Requirement already satisfied: datasets==2.20.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (2.20.0)\n",
      "Requirement already satisfied: sagemaker in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (2.232.1)\n",
      "Requirement already satisfied: sqlparse in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (0.5.1)\n",
      "Requirement already satisfied: SQLAlchemy in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (2.0.35)\n",
      "Collecting psycopg2 (from -r requirements.txt (line 20))\n",
      "  Downloading psycopg2-2.9.9.tar.gz (384 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyAthena in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (3.9.0)\n",
      "Requirement already satisfied: filelock in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 16)) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (3.10.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (0.25.1)\n",
      "Requirement already satisfied: packaging in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 16)) (6.0.2)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.27 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from boto3->-r requirements.txt (line 9)) (1.35.27)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from boto3->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from boto3->-r requirements.txt (line 9)) (0.10.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 12)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 12)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 12)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 12)) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 13)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 13)) (2024.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (2.2.1)\n",
      "Requirement already satisfied: docker in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (4.23.0)\n",
      "Requirement already satisfied: pathos in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (4.25.5)\n",
      "Requirement already satisfied: psutil in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (6.0.0)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (1.0.8)\n",
      "Requirement already satisfied: schema in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (3.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker->-r requirements.txt (line 17)) (2.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from SQLAlchemy->-r requirements.txt (line 19)) (4.12.2)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from PyAthena->-r requirements.txt (line 21)) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 16)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 16)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 16)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 16)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 16)) (1.12.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 17)) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 16)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 16)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 16)) (2024.8.30)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker->-r requirements.txt (line 17)) (2.9.2)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker->-r requirements.txt (line 17)) (13.8.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker->-r requirements.txt (line 17)) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 17)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 17)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 17)) (0.20.0)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from pathos->sagemaker->-r requirements.txt (line 17)) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from pathos->sagemaker->-r requirements.txt (line 17)) (0.3.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker->-r requirements.txt (line 17)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker->-r requirements.txt (line 17)) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker->-r requirements.txt (line 17)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker->-r requirements.txt (line 17)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/huthmac/Documents/AWS/00_workspace/text-to-sql-workshop/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker->-r requirements.txt (line 17)) (0.1.2)\n",
      "Building wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.9-cp311-cp311-macosx_13_0_arm64.whl size=144388 sha256=6896d60e5e48135d32b88bbffdc89d7d70fdf4108e89bca5a2308c4ba8c79937\n",
      "  Stored in directory: /Users/huthmac/Library/Caches/pip/wheels/ab/34/b9/78ebef1b3220b4840ee482461e738566c3c9165d2b5c914f51\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.9\n"
     ]
    }
   ],
   "source": [
    "# 2. Install dependencies\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default SageMaker S3 bucket: sagemaker-us-east-1-026459568683\n",
      "Using database: SQLALCHEMY with sql dialect: PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# 3. Import necessary libraries and load environment variables\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import boto3\n",
    "import sqlite3\n",
    "from pandas.io import sql\n",
    "from botocore.config import Config\n",
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "from io import StringIO\n",
    "import sqlparse\n",
    "import sqlite3\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import typing as t\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "os.environ['SQL_DATABASE'] = os.getenv('SQL_DATABASE') # LOCAL, SQLALCHEMY, REDSHIFT\n",
    "os.environ['SQL_DIALECT'] = os.getenv('SQL_DIALECT') # SQlite, PostgreSQL\n",
    "\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "SQL_DATABASE = os.environ['SQL_DATABASE']\n",
    "SQL_DIALECT = os.environ['SQL_DIALECT']\n",
    "\n",
    "\n",
    "# Create a SageMaker session\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get the default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Default SageMaker S3 bucket: {default_bucket}\")\n",
    "\n",
    "print(f\"Using database: {SQL_DATABASE} with sql dialect: {SQL_DIALECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Definition of helper classes\n",
    "\n",
    "# Bedrock LLM Class\n",
    "class BedrockLLMWrapper():\n",
    "    def __init__(self,\n",
    "        model_id: str = 'anthropic.claude-3-haiku-20240307-v1:0', #'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "        top_k: int = 5,\n",
    "        top_p: int = 0.7,\n",
    "        temperature: float = 0.0,\n",
    "        max_token_count: int = 4000,\n",
    "        max_attempts: int = 3,\n",
    "        debug: bool = False\n",
    "\n",
    "    ):\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.temperature = temperature\n",
    "        self.max_token_count = max_token_count\n",
    "        self.max_attempts = max_attempts\n",
    "        self.debug = debug\n",
    "        config = Config(\n",
    "            retries = {\n",
    "                'max_attempts': 10,\n",
    "                'mode': 'standard'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", config=config, region_name=REGION)\n",
    "        \n",
    "    def generate(self,prompt):\n",
    "        if self.debug: \n",
    "            print('entered BedrockLLMWrapper generate')\n",
    "        attempt = 1\n",
    "\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "        messages = []\n",
    "        messages.append(message)\n",
    "        \n",
    "        # model specific inference parameters to use.\n",
    "        if \"anthropic\" in self.model_id.lower():\n",
    "            # system_prompts = [{\"text\": \"You are a helpful AI Assistant.\"}]\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                                \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "                                \"topP\": self.top_p,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "        else:\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "\n",
    "        if self.debug: \n",
    "            print(\"Sending:\\nSystem:\\n\",system,\"\\nMessages:\\n\",str(messages))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                # Send the message.\n",
    "                response = self.bedrock_runtime.converse(\n",
    "                    modelId=self.model_id,\n",
    "                    messages=messages,\n",
    "                    system=system_prompts,\n",
    "                    inferenceConfig=inference_config,\n",
    "                    additionalModelRequestFields=additional_model_fields\n",
    "                )\n",
    "\n",
    "                # Log token usage.\n",
    "                text = response['output'].get('message').get('content')[0].get('text')\n",
    "                usage = response['usage']\n",
    "                latency = response['metrics'].get('latencyMs')\n",
    "\n",
    "                if self.debug: \n",
    "                    print(f'text: {text} ; and token usage: {usage} ; and query_time: {latency}')    \n",
    "                \n",
    "                break\n",
    "               \n",
    "            except Exception as e:\n",
    "                print(\"Error with calling Bedrock: \"+str(e))\n",
    "                attempt+=1\n",
    "                if attempt>self.max_attempts:\n",
    "                    print(\"Max attempts reached!\")\n",
    "                    result_text = str(e)\n",
    "                    break\n",
    "                else:#retry in 10 seconds\n",
    "                    print(\"retry\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "        # return result_text\n",
    "        return [text,usage,latency]\n",
    "\n",
    "     # Threaded function for queue processing.\n",
    "    def thread_request(self, q, results):\n",
    "        while True:\n",
    "            try:\n",
    "                index, prompt = q.get(block=False)\n",
    "                data = self.generate(prompt)\n",
    "                results[index] = data\n",
    "            except Queue.Empty:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'Error with prompt: {str(e)}')\n",
    "                results[index] = str(e)\n",
    "            finally:\n",
    "                q.task_done()\n",
    "\n",
    "    def generate_threaded(self, prompts, max_workers=15):\n",
    "        results = [None] * len(prompts)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_index = {executor.submit(self.generate, prompt): i for i, prompt in enumerate(prompts)}\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    results[index] = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'Generated an exception: {exc}')\n",
    "                    results[index] = str(exc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Utility class to get cost and visualizations \n",
    "class Util():\n",
    "    def __init__(self,\n",
    "        debug: bool = False\n",
    "\n",
    "    ):\n",
    "        self.debug = debug\n",
    "    \n",
    "    SCORE_PATTERN = r'<score>(.*?)</score>'\n",
    "    REASONING_PATTERN = r'<thinking>(.*?)</thinking>'\n",
    "    SQL_PATTERN = r'<[sS][qQ][lL]>(.*?)</[sS][qQ][lL]>'\n",
    "    DIFFICULTY_PATTERN = r'<difficulty>(.*?)</difficulty>'\n",
    "    USER_QUESTION_PATTERN = r'<user_question>(.*?)</user_question>'\n",
    "    SQL_DATABASE_SCHEMA_PATTERN = r'<sql_database_schema>(.*?)</sql_database_schema>'\n",
    "    SQL_DIALECT_PATTERN = r'<sql_dialect>(.*?)</sql_dialect>'\n",
    "\n",
    "\n",
    "    def compare_results(self, answer_results1, answer_results2):\n",
    "\n",
    "\n",
    "        # # Function to convert 'score' column\n",
    "        def convert_score(df):\n",
    "            # df['score'] = df['score'].map({'correct': 1, 'incorrect': 0})\n",
    "            df['score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0).astype(int)\n",
    "            return df\n",
    "\n",
    "        # Apply the conversion to both dataframes\n",
    "        answer_results1 = convert_score(answer_results1)\n",
    "        answer_results2 = convert_score(answer_results2)\n",
    "\n",
    "        # Calculate the average values for each metric\n",
    "        metrics = ['score', 'latency' ,'cost', 'ex_score', 'em_score','ves_score']\n",
    "        \n",
    "        avg_results1 = [answer_results1[metric].mean() for metric in metrics]\n",
    "        avg_results2 = [answer_results2[metric].mean() for metric in metrics]\n",
    "\n",
    "        # Calculate percentage change, handling divide-by-zero and infinite cases\n",
    "        def safe_percent_change(a, b):\n",
    "            if pd.isna(a) or pd.isna(b):\n",
    "                return 0\n",
    "            if a == 0 and b == 0:\n",
    "                return 0\n",
    "            elif a == 0:\n",
    "                return 100  # Arbitrarily set to 100% increase if original value was 0\n",
    "            else:\n",
    "                change = (b - a) / a * 100\n",
    "                return change if np.isfinite(change) else 0\n",
    "\n",
    "        percent_change = [safe_percent_change(a, b) for a, b in zip(avg_results1, avg_results2)]\n",
    "\n",
    "        # Set up the bar chart\n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.5\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Create the bars\n",
    "        bars = ax.bar(x, percent_change, width)\n",
    "\n",
    "        # Customize the chart\n",
    "        ax.set_ylabel('Percentage Change (%)')\n",
    "        ax.set_title('Percentage Change in Metrics (Results 2 vs Results 1)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(metrics)\n",
    "\n",
    "        # Add a horizontal line at y=0\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "        # Add value labels on top of each bar\n",
    "        def autolabel(rects):\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "                ax.annotate(f'{height:.2f}%',\n",
    "                            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                            xytext=(0, 3 if height >= 0 else -3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom' if height >= 0 else 'top')\n",
    "\n",
    "        autolabel(bars)\n",
    "\n",
    "        # Color the bars based on positive (green) or negative (red) change\n",
    "        # For latency & cost, reverse the color logic\n",
    "        for bar, change, metric in zip(bars, percent_change, metrics):\n",
    "            if metric == 'latency' or metric == 'cost':\n",
    "                bar.set_color('green' if change <= 0 else 'red')\n",
    "            else:\n",
    "                bar.set_color('green' if change >= 0 else 'red')\n",
    "            \n",
    "\n",
    "        # Adjust layout and display the chart\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_distribution(self, df, key):\n",
    "        # Check if 'score' column exists in the DataFrame\n",
    "        if key not in df.columns:\n",
    "            raise ValueError(f\"The DataFrame does not contain a '{key}' column.\")\n",
    "        \n",
    "        # Count the frequency of each score\n",
    "        score_counts = df[key].value_counts().sort_index()\n",
    "        \n",
    "        # Create a bar chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(score_counts.index, score_counts.values)\n",
    "        \n",
    "        # Customize the chart\n",
    "        plt.title(f'Distribution of {key}')\n",
    "        plt.xlabel(f'{key}')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(range(int(score_counts.index.min()), int(score_counts.index.max()) + 1))\n",
    "        \n",
    "        # Add value labels on top of each bar\n",
    "        for i, v in enumerate(score_counts.values):\n",
    "            plt.text(score_counts.index[i], v, str(v), ha='center', va='bottom')\n",
    "        \n",
    "        # Display the chart\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Strip out the portion of the response with regex.\n",
    "    def extract_with_regex(self, response, regex):\n",
    "        matches = re.search(regex, response, re.DOTALL)\n",
    "        # Extract the matched content, if any\n",
    "        return matches.group(1).strip() if matches else None\n",
    "\n",
    "    def calculate_cost(self, usage, model_id):\n",
    "        '''\n",
    "        Takes the usage tokens returned by Bedrock in input and output, and coverts to cost in dollars.\n",
    "        '''\n",
    "        \n",
    "        input_token_haiku = 0.25/1000000\n",
    "        output_token_haiku = 1.25/1000000\n",
    "        input_token_sonnet = 3.00/1000000\n",
    "        output_token_sonnet = 15.00/1000000\n",
    "        input_token_opus = 15.00/1000000\n",
    "        output_token_opus = 75.00/1000000\n",
    "        \n",
    "        input_token_titan_embeddingv1 = 0.1/1000000\n",
    "        input_token_titan_embeddingv2 = 0.02/1000000\n",
    "        input_token_titan_embeddingmultimodal = 0.8/1000000\n",
    "        input_token_titan_premier = 0.5/1000000\n",
    "        output_token_titan_premier = 1.5/1000000\n",
    "        input_token_titan_lite = 0.15/1000000\n",
    "        output_token_titan_lite = 0.2/1000000\n",
    "        input_token_titan_express = 0.2/1000000\n",
    "        output_token_titan_express = 0.6/1000000\n",
    "       \n",
    "        input_token_cohere_command = 0.15/1000000\n",
    "        output_token_cohere_command = 2/1000000\n",
    "        input_token_cohere_commandlight = 0.3/1000000\n",
    "        output_token_cohere_commandlight = 0.6/1000000\n",
    "        input_token_cohere_commandrplus = 3/1000000\n",
    "        output_token_cohere_commandrplus = 15/1000000\n",
    "        input_token_cohere_commandr = 5/1000000\n",
    "        output_token_cohere_commandr = 1.5/1000000\n",
    "        input_token_cohere_embedenglish = 0.1/1000000\n",
    "        input_token_cohere_embedmultilang = 0.1/1000000\n",
    "\n",
    "        input_token_llama3_8b = 0.4/1000000\n",
    "        output_token_llama3_8b = 0.6/1000000\n",
    "        input_token_llama3_70b = 2.6/1000000\n",
    "        output_token_llama3_70b = 3.5/1000000\n",
    "\n",
    "        input_token_mistral_8b = 0.15/1000000\n",
    "        output_token_mistral_8b = 0.2/1000000\n",
    "        input_token_mistral_large = 4/1000000\n",
    "        output_token_mistral_large = 12/1000000\n",
    "\n",
    "        cost = 0\n",
    "\n",
    "        if 'haiku' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_haiku\n",
    "            cost+= usage['outputTokens']*output_token_haiku\n",
    "        if 'sonnet' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_sonnet\n",
    "            cost+= usage['outputTokens']*output_token_sonnet\n",
    "        if 'opus' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_opus\n",
    "            cost+= usage['outputTokens']*output_token_opus\n",
    "        if 'amazon.titan-embed-text-v1' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_embeddingv1\n",
    "        if 'amazon.titan-embed-text-v2' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_embeddingv2\n",
    "        if 'cohere.embed-multilingual' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_embedmultilang\n",
    "        if 'cohere.embed-english' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_embedenglish \n",
    "        if 'meta.llama3-8b-instruct' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_llama3_8b\n",
    "            cost+= usage['outputTokens']*output_token_llama3_8b\n",
    "        if 'meta.llama3-70b-instruct' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_llama3_70b\n",
    "            cost+= usage['outputTokens']*output_token_llama3_70b\n",
    "        if 'cohere.command-text' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_command\n",
    "            cost+= usage['outputTokens']*output_token_cohere_command\n",
    "        if 'cohere.command-light-text' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandlight\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandlight\n",
    "        if 'cohere.command-r-plus' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandrplus\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandrplus\n",
    "        if 'cohere.command-r' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandr\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandr\n",
    "        if 'amazon.titan-text-express' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_express\n",
    "            cost+= usage['outputTokens']*output_token_titan_express\n",
    "        if 'amazon.titan-text-lite' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_lite\n",
    "            cost+= usage['outputTokens']*output_token_titan_lite\n",
    "        if 'amazon.titan-text-premier' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_premier\n",
    "            cost+= usage['outputTokens']*output_token_titan_premier\n",
    "        if 'mistral.mixtral-8x7b-instruct-v0:1' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_mistral_8b\n",
    "            cost+= usage['outputTokens']*output_token_mistral_8b\n",
    "\n",
    "        return cost\n",
    "\n",
    "# Utility class to get database schema, create tables, and run SQL queries\n",
    "import requests\n",
    "import sqlite3\n",
    "import re\n",
    "from pyathena import connect\n",
    "# import psycopg2\n",
    "from sqlalchemy import create_engine, MetaData, text\n",
    "\n",
    "\n",
    "class DatabaseUtil():\n",
    "    def __init__(self,\n",
    "        debug: bool = False,\n",
    "        datasource_url: [] = ['https://d3q8adh3y5sxpk.cloudfront.net/sql-workshop/data/redshift-sourcedb.sql'],\n",
    "        sql_database: str = 'LOCAL',\n",
    "        sql_database_name: str = 'dev',\n",
    "        region: str = 'us-east-1',\n",
    "        s3_bucketname: str = ''\n",
    "\n",
    "    ):\n",
    "        self.debug = debug\n",
    "        self.datasource_url = datasource_url\n",
    "        self.sql_database = sql_database\n",
    "        self.sql_database_name = sql_database_name\n",
    "        self.region = region\n",
    "        self.s3_bucketname = s3_bucketname\n",
    "\n",
    "    # retrieve AWS secret for database connection\n",
    "    def get_secret(self, secret_name):\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(service_name='secretsmanager', region_name=self.region)\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "        return get_secret_value_response\n",
    "\n",
    "    def get_table_reflections(self, engine) -> MetaData:\n",
    "    \n",
    "        # Instantiate MetaData object\n",
    "        metadata = MetaData()\n",
    "        \n",
    "        # Reflect the database schema with the engine\n",
    "        metadata.reflect(bind=engine)\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def convert_reflection_to_dict(self, metadata: MetaData) -> dict:\n",
    "        table_definitions: list[dict] = []\n",
    "        for table_name in metadata.tables:\n",
    "            definition = {}\n",
    "            definition['table'] = table_name\n",
    "            # The metadata.table[x].columns value is type sqlalchemy.sql.base.ReadOnlyColumnCollection\n",
    "            # Lets convert it into something more usable. c.type returns a SQLAlchemy object so we convert to string.\n",
    "            definition['columns'] = { c.name: str(c.type) for c in metadata.tables[table_name].columns }\n",
    "        \n",
    "            table_header = f\"Table: {table_name}\"\n",
    "            columns_definition = '\\n'.join([f\"Column: {c.name}, Type: {c.type}\" for c in metadata.tables[table_name].columns])\n",
    "            string_representation = f\"{table_header}\\n{columns_definition}\"\n",
    "        \n",
    "            definition['string_representation'] = string_representation\n",
    "        \n",
    "            table_definitions.append(definition)\n",
    "        \n",
    "        \n",
    "        # The metadata table is a FacadeDict object which is immutable so we need to remove unwanted tables in the new list.\n",
    "        table_names_to_exclude = set(['table_embedding', 'alembic_version'])\n",
    "        table_definitions = [d for d in table_definitions if d['table'] not in table_names_to_exclude]\n",
    "\n",
    "        return table_definitions\n",
    "    \n",
    "    def create_database_tables(self):\n",
    "        # Download the SQL files\n",
    "        \n",
    "            # create local db and import northwind database\n",
    "            for url in self.datasource_url:\n",
    "                response = requests.get(url)\n",
    "                sql_content = response.text\n",
    "                # Split the SQL content into individual statements\n",
    "                sql_statements = re.split(r';\\s*$', sql_content, flags=re.MULTILINE)\n",
    "                \n",
    "                if self.sql_database == 'LOCAL':\n",
    "                    try:\n",
    "                        # Create a SQLite database connection\n",
    "                        conn = sqlite3.connect('devdb.db')\n",
    "                        cursor = conn.cursor()\n",
    "\n",
    "                        # Execute each SQL statement\n",
    "                        for statement in sql_statements:\n",
    "                            # Skip empty statements\n",
    "                            if statement.strip():\n",
    "                                # print(f'statement: {statement}')\n",
    "                                # Replace PostgreSQL-specific syntax with SQLite equivalents\n",
    "                                statement = statement.replace('SERIAL PRIMARY KEY', 'INTEGER PRIMARY KEY AUTOINCREMENT')\n",
    "                                statement = statement.replace('::int', '')\n",
    "                                statement = statement.replace('::varchar', '')\n",
    "                                statement = statement.replace('::real', '')\n",
    "                                statement = statement.replace('::date', '')\n",
    "                                statement = statement.replace('::boolean', '')\n",
    "                                statement = statement.replace('public.', '')\n",
    "                                statement = re.sub(r'WITH \\(.*?\\)', '', statement)\n",
    "                                \n",
    "                                try:\n",
    "                                    cursor.execute(statement)\n",
    "                                except sqlite3.Error as e:\n",
    "                                    print(f\"Error executing statement: {e}\")\n",
    "\n",
    "                        # Commit the changes and close the connection\n",
    "                        conn.commit()\n",
    "                        conn.close()\n",
    "\n",
    "                        print(\"SQL execution completed.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error creating tables: {e}\")\n",
    "                        raise\n",
    "\n",
    "                if self.sql_database == 'REDSHIFT':\n",
    "                    try:\n",
    "                        rdc = boto3.client('redshift-data')\n",
    "                        get_secret_value_response = self.get_secret(\"RedshiftCreds\")\n",
    "                        # parse REDSHIFT_CLUSTER_DETAILS to extract WorkgroupName, Database, DbUser\n",
    "                        WorkgroupName = json.loads(get_secret_value_response['SecretString']).get('workgroupname')\n",
    "                        Database = json.loads(get_secret_value_response['SecretString']).get('workgroupname')\n",
    "                        DbUser = json.loads(get_secret_value_response['SecretString']).get('username')\n",
    "\n",
    "                        for statement in sql_statements:\n",
    "                            try:        \n",
    "                                rdc.execute_statement(\n",
    "                                    WorkgroupName=WorkgroupName,\n",
    "                                    Database=Database,\n",
    "                                    DbUser=DbUser,\n",
    "                                    Sql=statement\n",
    "                                )\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"Error executing statement: {e}\")\n",
    "                        print(\"SQL execution completed.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error creating tables: {e}\")\n",
    "                        raise\n",
    "                \n",
    "                if self.sql_database =='SQLALCHEMY':\n",
    "                    # create tables in database\n",
    "                    try:\n",
    "                        # SQLALCHEMY_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{SQL_DATABASE_NAME}\"\n",
    "                        get_secret_value_response = self.get_secret(\"SQLALCHEMY_URL\")\n",
    "                        SQLALCHEMY_URL = get_secret_value_response['SecretString']\n",
    "\n",
    "                        print(f\"SQLALCHEMY_URL: {SQLALCHEMY_URL}\")\n",
    "                        engine = create_engine(SQLALCHEMY_URL)\n",
    "                        with engine.connect() as connection:\n",
    "                            # Execute each SQL statement\n",
    "                            for statement in sql_statements:\n",
    "                                # Skip empty statements\n",
    "                                if statement.strip():\n",
    "                                    # print(f'statement: {statement}')\n",
    "                                    # Replace PostgreSQL-specific syntax with SQLite equivalents\n",
    "                                    statement = statement.replace('SERIAL PRIMARY KEY', 'INTEGER PRIMARY KEY AUTOINCREMENT')\n",
    "                                    statement = statement.replace('::int', '')\n",
    "                                    statement = statement.replace('::varchar', '')\n",
    "                                    statement = statement.replace('::real', '')\n",
    "                                    statement = statement.replace('::date', '')\n",
    "                                    statement = statement.replace('::boolean', '')\n",
    "                                    statement = statement.replace('public.', '')\n",
    "                                    statement = statement.replace('VARBYTE', 'bytea')\n",
    "                                    statement = statement.replace('bpchar', 'varchar')\n",
    "                                    \n",
    "                                    statement = re.sub(r'WITH \\(.*?\\)', '', statement)\n",
    "                                    \n",
    "                                    try:\n",
    "                                        connection.execute(text(statement))\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Error executing statement: {e}\")\n",
    "                            connection.commit()\n",
    "                            print(\"SQL execution completed.\")    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error creating tables: {e}\")\n",
    "                        raise\n",
    "\n",
    "\n",
    "    def get_schema_as_string(self):\n",
    "        if self.sql_database == 'LOCAL':\n",
    "            db_path = 'devdb.db'          \n",
    "            conn = sqlite3.connect(db_path)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Query to get all table names\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            tables = cursor.fetchall()\n",
    "\n",
    "            schema_string = \"\"\n",
    "\n",
    "            for table in tables:\n",
    "                table_name = table[0]\n",
    "                # Query to get the CREATE TABLE statement for each table\n",
    "                cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table_name}';\")\n",
    "                create_table_stmt = cursor.fetchone()[0]\n",
    "                \n",
    "                schema_string += f\"{create_table_stmt};\\n\\n\"\n",
    "\n",
    "            conn.close()\n",
    "            return schema_string\n",
    "\n",
    "        if self.sql_database =='SQLALCHEMY':\n",
    "            try:\n",
    "                # Use SQLAlchemy if SQL Alchemy is used\n",
    "                # SQLALCHEMY_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{SQL_DATABASE_NAME}\"\n",
    "                get_secret_value_response = self.get_secret(\"SQLALCHEMY_URL\")\n",
    "                SQLALCHEMY_URL = get_secret_value_response['SecretString']\n",
    "                \n",
    "                engine = create_engine(SQLALCHEMY_URL)\n",
    "                metadata = self.get_table_reflections(engine)\n",
    "                table_definitions = self.convert_reflection_to_dict(metadata)\n",
    "\n",
    "                return table_definitions\n",
    "                # with engine.connect() as connection:\n",
    "                #     result = connection.execute(\"\"\"\"\"\")\n",
    "                #     return result.fetchall()\n",
    "            except Exception as e:\n",
    "                error = f\"Error executing statement: {e}\"\n",
    "                print(error)\n",
    "\n",
    "        if self.sql_database == \"REDSHIFT\":\n",
    "            try:\n",
    "                get_secret_value_response = self.get_secret(\"RedshiftCreds\")\n",
    "                # parse REDSHIFT_CLUSTER_DETAILS to extract WorkgroupName, Database, DbUser\n",
    "                WorkgroupName = json.loads(get_secret_value_response['SecretString']).get('workgroupname')\n",
    "                Database = json.loads(get_secret_value_response['SecretString']).get('workgroupname')\n",
    "                DbUser = json.loads(get_secret_value_response['SecretString']).get('username')\n",
    "                \n",
    "                rdc = boto3.client('redshift-data')\n",
    "                result = rdc.execute_statement(\n",
    "                    WorkgroupName=WorkgroupName,\n",
    "                    Database=Database,\n",
    "                    DbUser=DbUser,\n",
    "                    Sql=f\"select * from pg_table_def where schemaname = 'public';\"\n",
    "                )\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing statement: {e}\")\n",
    "      \n",
    "            \n",
    "        if self.sql_database == 'GLUE':\n",
    "            # use a Glue database\n",
    "            table_names=None\n",
    "            try:\n",
    "                glue_client = boto3.client('glue', region_name=self.region)\n",
    "                table_schema_list = []\n",
    "                response = glue_client.get_tables(DatabaseName=self.sql_database_name)\n",
    "\n",
    "                all_table_names = [table['Name'] for table in response['TableList']]\n",
    "\n",
    "                if table_names:\n",
    "                    table_names = [name for name in table_names if name in all_table_names]\n",
    "                else:\n",
    "                    table_names = all_table_names\n",
    "\n",
    "                for table_name in table_names:\n",
    "                    response = glue_client.get_table(DatabaseName=self.sql_database_name, Name=table_name)\n",
    "                    columns = response['Table']['StorageDescriptor']['Columns']\n",
    "                    schema = {column['Name']: column['Type'] for column in columns}\n",
    "                    table_schema_list.append({\"Table: {}\".format(table_name): 'Schema: {}'.format(schema)})\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "            return table_schema_list\n",
    "        \n",
    "    def run_sql(self, statement):\n",
    "    \n",
    "        if self.sql_database == 'LOCAL':\n",
    "            try:\n",
    "                # Create a SQLite database connection\n",
    "                conn = sqlite3.connect('devdb.db')\n",
    "                cursor = conn.cursor()\n",
    "\n",
    "                cursor.execute(statement)\n",
    "                # Fetch all rows from the result\n",
    "                result = cursor.fetchall()\n",
    "                conn.close()\n",
    "                return result\n",
    "            except sqlite3.Error as e:\n",
    "                error = f\"Error executing statement: {e}\"\n",
    "                raise\n",
    "            \n",
    "            finally:\n",
    "                conn.close()\n",
    "                \n",
    "        if self.sql_database == 'GLUE':\n",
    "            try:\n",
    "                # Use Athena if AWS Glue Schema is used\n",
    "                athenacursor = connect(s3_staging_dir=f\"s3://{self.s3_bucketname}/athena/\",\n",
    "                                        region_name=self.region).cursor()\n",
    "                athenacursor.execute(statement)\n",
    "                result = pd.DataFrame(athenacursor.fetchall()).to_string(index=False)\n",
    "                # convert df to string\n",
    "                return result\n",
    "            \n",
    "            except Exception as e:\n",
    "                error = f\"Error executing statement: {e}\"\n",
    "                raise\n",
    "        \n",
    "        if self.sql_database == \"REDSHIFT\":\n",
    "            try:\n",
    "                get_secret_value_response = self.get_secret(\"RedshiftCreds\")\n",
    "                # parse REDSHIFT_CLUSTER_DETAILS to extract WorkgroupName, Database, DbUser\n",
    "                WorkgroupName = json.loads(get_secret_value_response['SecretString']).get('workgroupname')\n",
    "                Database = json.loads(get_secret_value_response['SecretString']).get('workgroupname')\n",
    "                DbUser = json.loads(get_secret_value_response['SecretString']).get('username')\n",
    "\n",
    "                rdc = boto3.client('redshift-data')\n",
    "                result = rdc.execute_statement(\n",
    "                    WorkgroupName=WorkgroupName,\n",
    "                    Database=Database,\n",
    "                    DbUser=DbUser,\n",
    "                    Sql=statement\n",
    "                )\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error executing statement: {e}\")\n",
    "\n",
    "        if self.sql_database =='SQLALCHEMY':\n",
    "            try:\n",
    "                # SQLALCHEMY_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{SQL_DATABASE_NAME}\"\n",
    "                get_secret_value_response = self.get_secret(\"SQLALCHEMY_URL\")\n",
    "                SQLALCHEMY_URL = get_secret_value_response['SecretString']\n",
    "                \n",
    "                engine = create_engine(SQLALCHEMY_URL)\n",
    "                with engine.connect() as connection:\n",
    "                    result = connection.execute(text(statement))\n",
    "                    return result.fetchall()\n",
    "            except Exception as e:\n",
    "                error = f\"Error executing statement: {e}\"\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLALCHEMY_URL: postgresql://masteruser:rkKpkHN982shTOqf@workshop-database.cluster-cehkbhhtrbhr.us-east-1.rds.amazonaws.com:5432/dev\n",
      "SQL execution completed.\n",
      "None\n",
      "[{'table': 'customer_demographics', 'columns': {'customer_type_id': 'VARCHAR', 'customer_desc': 'TEXT'}, 'string_representation': 'Table: customer_demographics\\nColumn: customer_type_id, Type: VARCHAR\\nColumn: customer_desc, Type: TEXT'}, {'table': 'customer_customer_demo', 'columns': {'customer_id': 'VARCHAR', 'customer_type_id': 'VARCHAR'}, 'string_representation': 'Table: customer_customer_demo\\nColumn: customer_id, Type: VARCHAR\\nColumn: customer_type_id, Type: VARCHAR'}, {'table': 'customers', 'columns': {'customer_id': 'VARCHAR', 'company_name': 'VARCHAR(40)', 'contact_name': 'VARCHAR(30)', 'contact_title': 'VARCHAR(30)', 'address': 'VARCHAR(60)', 'city': 'VARCHAR(15)', 'region': 'VARCHAR(15)', 'postal_code': 'VARCHAR(10)', 'country': 'VARCHAR(15)', 'phone': 'VARCHAR(24)', 'fax': 'VARCHAR(24)'}, 'string_representation': 'Table: customers\\nColumn: customer_id, Type: VARCHAR\\nColumn: company_name, Type: VARCHAR(40)\\nColumn: contact_name, Type: VARCHAR(30)\\nColumn: contact_title, Type: VARCHAR(30)\\nColumn: address, Type: VARCHAR(60)\\nColumn: city, Type: VARCHAR(15)\\nColumn: region, Type: VARCHAR(15)\\nColumn: postal_code, Type: VARCHAR(10)\\nColumn: country, Type: VARCHAR(15)\\nColumn: phone, Type: VARCHAR(24)\\nColumn: fax, Type: VARCHAR(24)'}, {'table': 'employees', 'columns': {'employee_id': 'SMALLINT', 'last_name': 'VARCHAR(20)', 'first_name': 'VARCHAR(10)', 'title': 'VARCHAR(30)', 'title_of_courtesy': 'VARCHAR(25)', 'birth_date': 'DATE', 'hire_date': 'DATE', 'address': 'VARCHAR(60)', 'city': 'VARCHAR(15)', 'region': 'VARCHAR(15)', 'postal_code': 'VARCHAR(10)', 'country': 'VARCHAR(15)', 'home_phone': 'VARCHAR(24)', 'extension': 'VARCHAR(4)', 'photo': 'BYTEA', 'notes': 'TEXT', 'reports_to': 'SMALLINT', 'photo_path': 'VARCHAR(255)'}, 'string_representation': 'Table: employees\\nColumn: employee_id, Type: SMALLINT\\nColumn: last_name, Type: VARCHAR(20)\\nColumn: first_name, Type: VARCHAR(10)\\nColumn: title, Type: VARCHAR(30)\\nColumn: title_of_courtesy, Type: VARCHAR(25)\\nColumn: birth_date, Type: DATE\\nColumn: hire_date, Type: DATE\\nColumn: address, Type: VARCHAR(60)\\nColumn: city, Type: VARCHAR(15)\\nColumn: region, Type: VARCHAR(15)\\nColumn: postal_code, Type: VARCHAR(10)\\nColumn: country, Type: VARCHAR(15)\\nColumn: home_phone, Type: VARCHAR(24)\\nColumn: extension, Type: VARCHAR(4)\\nColumn: photo, Type: BYTEA\\nColumn: notes, Type: TEXT\\nColumn: reports_to, Type: SMALLINT\\nColumn: photo_path, Type: VARCHAR(255)'}, {'table': 'categories', 'columns': {'category_id': 'SMALLINT', 'category_name': 'VARCHAR(15)', 'description': 'TEXT', 'picture': 'BYTEA'}, 'string_representation': 'Table: categories\\nColumn: category_id, Type: SMALLINT\\nColumn: category_name, Type: VARCHAR(15)\\nColumn: description, Type: TEXT\\nColumn: picture, Type: BYTEA'}, {'table': 'products', 'columns': {'product_id': 'SMALLINT', 'product_name': 'VARCHAR(40)', 'supplier_id': 'SMALLINT', 'category_id': 'SMALLINT', 'quantity_per_unit': 'VARCHAR(20)', 'unit_price': 'REAL', 'units_in_stock': 'SMALLINT', 'units_on_order': 'SMALLINT', 'reorder_level': 'SMALLINT', 'discontinued': 'INTEGER'}, 'string_representation': 'Table: products\\nColumn: product_id, Type: SMALLINT\\nColumn: product_name, Type: VARCHAR(40)\\nColumn: supplier_id, Type: SMALLINT\\nColumn: category_id, Type: SMALLINT\\nColumn: quantity_per_unit, Type: VARCHAR(20)\\nColumn: unit_price, Type: REAL\\nColumn: units_in_stock, Type: SMALLINT\\nColumn: units_on_order, Type: SMALLINT\\nColumn: reorder_level, Type: SMALLINT\\nColumn: discontinued, Type: INTEGER'}, {'table': 'suppliers', 'columns': {'supplier_id': 'SMALLINT', 'company_name': 'VARCHAR(40)', 'contact_name': 'VARCHAR(30)', 'contact_title': 'VARCHAR(30)', 'address': 'VARCHAR(60)', 'city': 'VARCHAR(15)', 'region': 'VARCHAR(15)', 'postal_code': 'VARCHAR(10)', 'country': 'VARCHAR(15)', 'phone': 'VARCHAR(24)', 'fax': 'VARCHAR(24)', 'homepage': 'TEXT'}, 'string_representation': 'Table: suppliers\\nColumn: supplier_id, Type: SMALLINT\\nColumn: company_name, Type: VARCHAR(40)\\nColumn: contact_name, Type: VARCHAR(30)\\nColumn: contact_title, Type: VARCHAR(30)\\nColumn: address, Type: VARCHAR(60)\\nColumn: city, Type: VARCHAR(15)\\nColumn: region, Type: VARCHAR(15)\\nColumn: postal_code, Type: VARCHAR(10)\\nColumn: country, Type: VARCHAR(15)\\nColumn: phone, Type: VARCHAR(24)\\nColumn: fax, Type: VARCHAR(24)\\nColumn: homepage, Type: TEXT'}, {'table': 'orders', 'columns': {'order_id': 'SMALLINT', 'customer_id': 'VARCHAR', 'employee_id': 'SMALLINT', 'order_date': 'DATE', 'required_date': 'DATE', 'shipped_date': 'DATE', 'ship_via': 'SMALLINT', 'freight': 'REAL', 'ship_name': 'VARCHAR(40)', 'ship_address': 'VARCHAR(60)', 'ship_city': 'VARCHAR(15)', 'ship_region': 'VARCHAR(15)', 'ship_postal_code': 'VARCHAR(10)', 'ship_country': 'VARCHAR(15)'}, 'string_representation': 'Table: orders\\nColumn: order_id, Type: SMALLINT\\nColumn: customer_id, Type: VARCHAR\\nColumn: employee_id, Type: SMALLINT\\nColumn: order_date, Type: DATE\\nColumn: required_date, Type: DATE\\nColumn: shipped_date, Type: DATE\\nColumn: ship_via, Type: SMALLINT\\nColumn: freight, Type: REAL\\nColumn: ship_name, Type: VARCHAR(40)\\nColumn: ship_address, Type: VARCHAR(60)\\nColumn: ship_city, Type: VARCHAR(15)\\nColumn: ship_region, Type: VARCHAR(15)\\nColumn: ship_postal_code, Type: VARCHAR(10)\\nColumn: ship_country, Type: VARCHAR(15)'}, {'table': 'shippers', 'columns': {'shipper_id': 'SMALLINT', 'company_name': 'VARCHAR(40)', 'phone': 'VARCHAR(24)'}, 'string_representation': 'Table: shippers\\nColumn: shipper_id, Type: SMALLINT\\nColumn: company_name, Type: VARCHAR(40)\\nColumn: phone, Type: VARCHAR(24)'}, {'table': 'region', 'columns': {'region_id': 'SMALLINT', 'region_description': 'VARCHAR'}, 'string_representation': 'Table: region\\nColumn: region_id, Type: SMALLINT\\nColumn: region_description, Type: VARCHAR'}, {'table': 'territories', 'columns': {'territory_id': 'VARCHAR(20)', 'territory_description': 'VARCHAR', 'region_id': 'SMALLINT'}, 'string_representation': 'Table: territories\\nColumn: territory_id, Type: VARCHAR(20)\\nColumn: territory_description, Type: VARCHAR\\nColumn: region_id, Type: SMALLINT'}, {'table': 'employee_territories', 'columns': {'employee_id': 'SMALLINT', 'territory_id': 'VARCHAR(20)'}, 'string_representation': 'Table: employee_territories\\nColumn: employee_id, Type: SMALLINT\\nColumn: territory_id, Type: VARCHAR(20)'}, {'table': 'order_details', 'columns': {'order_id': 'SMALLINT', 'product_id': 'SMALLINT', 'unit_price': 'REAL', 'quantity': 'SMALLINT', 'discount': 'REAL'}, 'string_representation': 'Table: order_details\\nColumn: order_id, Type: SMALLINT\\nColumn: product_id, Type: SMALLINT\\nColumn: unit_price, Type: REAL\\nColumn: quantity, Type: SMALLINT\\nColumn: discount, Type: REAL'}, {'table': 'us_states', 'columns': {'state_id': 'SMALLINT', 'state_name': 'VARCHAR(100)', 'state_abbr': 'VARCHAR(2)', 'state_region': 'VARCHAR(50)'}, 'string_representation': 'Table: us_states\\nColumn: state_id, Type: SMALLINT\\nColumn: state_name, Type: VARCHAR(100)\\nColumn: state_abbr, Type: VARCHAR(2)\\nColumn: state_region, Type: VARCHAR(50)'}]\n"
     ]
    }
   ],
   "source": [
    "# 5. Initialize database wrapper to run sql queries, get database schema, and create tables in database\n",
    "\n",
    "if SQL_DATABASE == 'SQLALCHEMY':\n",
    "        # SQLALCHEMY\n",
    "        databaseutil = DatabaseUtil(\n",
    "                        datasource_url=[\"https://d3q8adh3y5sxpk.cloudfront.net/sql-workshop/data/redshift-sourcedb.sql\"],\n",
    "                        sql_database= 'SQLALCHEMY'\n",
    "        )\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "        # LOCAL SqlLite\n",
    "        databaseutil = DatabaseUtil(\n",
    "                        datasource_url=[\"https://d3q8adh3y5sxpk.cloudfront.net/sql-workshop/data/redshift-sourcedb.sql\"],\n",
    "                        sql_database= 'LOCAL'\n",
    "        )\n",
    "\n",
    "result = databaseutil.create_database_tables()\n",
    "print(result)\n",
    "           \n",
    "schema = databaseutil.get_schema_as_string()\n",
    "print(schema)\n",
    "\n",
    "# result = databaseutil.run_sql(\"SELECT * from public.customers\")\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to ./data/ground_truth.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 6. Download ground truth \n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# URL of the file to download\n",
    "url = \"https://d3q8adh3y5sxpk.cloudfront.net/sql-workshop/data/question_query_good_results.jsonl\"\n",
    "\n",
    "# Path to the local data folder\n",
    "data_folder = \"./data\"\n",
    "\n",
    "# Create the data folder if it doesn't exist\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# File name to save the downloaded file\n",
    "file_name = \"ground_truth.jsonl\"\n",
    "\n",
    "# Full path to save the file\n",
    "file_path = os.path.join(data_folder, file_name)\n",
    "\n",
    "# Send a GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the file to the local data folder\n",
    "with open(file_path, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(f\"File downloaded and saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df with column names: Index(['Question', 'Query', 'Result', 'Error', 'Context'], dtype='object') already exists in memory.\n",
      "Number of successful queries: 124\n",
      "Number of unsuccessful queries: 0\n"
     ]
    }
   ],
   "source": [
    "# 7. Validate/ensure ground truth SQL queries run successfully\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "results = []\n",
    "\n",
    "# Check if df exists in the current namespace\n",
    "if 'df' not in globals():\n",
    "    # If it doesn't exist, try to load it from a JSONL file\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the dataframe from the JSONL file\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        print(\"df loaded from JSONL file.\")\n",
    "    else:\n",
    "        print(f\"Error: JSONL file not found at {file_path}\")\n",
    "else:\n",
    "    print(f\"df with column names: {df.columns} already exists in memory.\")\n",
    "\n",
    "df.columns = df.columns.str.capitalize()\n",
    "\n",
    "\n",
    "for row in df.itertuples():\n",
    "    # print(row.query)\n",
    "    error = None\n",
    "    result = None\n",
    "    try:\n",
    "        \n",
    "        result = databaseutil.run_sql(row.Query)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error = e\n",
    "\n",
    "    results.append({'Question': row.Question,'Query': row.Query, 'Result': result, 'Error': error, 'Context': schema})\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_good_results = df_results[df_results['Error'].isnull() | (df_results['Error'] == None)]\n",
    "print(f\"Number of successful queries: {len(df_good_results)}\")\n",
    "\n",
    "df_bad_results = df_results[df_results['Error'].notnull() | (df_results['Error'] == 'None')]\n",
    "print(f\"Number of unsuccessful queries: {len(df_bad_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create Text-to-SQL zero-shot prompt\n",
    "# This function builds a text-to-SQL zero-shot prompt for a given user question and SQL database schema.\n",
    "# The prompt includes the original user question, the SQL database schema, and instructions for generating a SQL query.\n",
    "# The sql_dialect parameter specifies the SQL dialect to be used in the generated SQL query (e.g., MySQL, SQLite, etc.).\n",
    "\n",
    "\n",
    "# sql_dialect = awsathena or SQLite\n",
    "def build_sqlquerygen_prompt(user_question: str, sql_database_schema: str):\n",
    "    prompt = \"\"\"You are a SQL expert. You will be provided with the original user question and a SQL database schema. \n",
    "                Only return the SQL query and nothing else.\n",
    "                Here is the original user question.\n",
    "                <user_question>\n",
    "                {user_question}\n",
    "                </user_question>\n",
    "\n",
    "                Here is the SQL database schema.\n",
    "                <sql_database_schema>\n",
    "                {sql_database_schema}\n",
    "                </sql_database_schema>\n",
    "                \n",
    "                Instructions:\n",
    "                Generate a SQL query that answers the original user question.\n",
    "                Use the schema, first create a syntactically correct {sql_dialect} query to answer the question. \n",
    "                Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
    "                Always prefix table names with the \"public.\" prefix.\n",
    "                Pay attention to use only the column names that you can see in the schema description. \n",
    "                Be careful to not query for columns that do not exist. \n",
    "                Pay attention to which column is in which table. \n",
    "                Also, qualify column names with the table name when needed.\n",
    "                If you cannot answer the user question with the help of the provided SQL database schema, \n",
    "                then output that this question question cannot be answered based of the information stored in the database.\n",
    "                You are required to use the following format, each taking one line:\n",
    "                Return the sql query inside the <SQL></SQL> tab.\n",
    "                \"\"\".format(\n",
    "                    user_question=user_question,\n",
    "                    sql_database_schema=sql_database_schema,\n",
    "                    sql_dialect=SQL_DIALECT\n",
    "                ) \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Question  \\\n",
      "0         What is the total number of customers?   \n",
      "1  List all product names and their unit prices.   \n",
      "2    Who are the top 5 customers by order count?   \n",
      "\n",
      "                                               Query  \\\n",
      "0            SELECT COUNT(*)\\nFROM public.customers;   \n",
      "1  SELECT products.product_name, products.unit_pr...   \n",
      "2  SELECT c.customer_id, COUNT(o.order_id) as ord...   \n",
      "\n",
      "                                              Result Error  \\\n",
      "0                                             [(91)]  None   \n",
      "1  [(Chai, 18.0), (Chang, 19.0), (Aniseed Syrup, ...  None   \n",
      "2  [(SAVEA, 31), (ERNSH, 30), (QUICK, 28), (HUNGO...  None   \n",
      "\n",
      "                                      ReferenceQuery  \\\n",
      "0                    SELECT COUNT(*) FROM customers;   \n",
      "1     SELECT product_name, unit_price FROM products;   \n",
      "2  SELECT c.company_name, COUNT(o.order_id) as or...   \n",
      "\n",
      "                                             Context  \n",
      "0  [{'table': 'customer_demographics', 'columns':...  \n",
      "1  [{'table': 'customer_demographics', 'columns':...  \n",
      "2  [{'table': 'customer_demographics', 'columns':...  \n",
      "Number of successful queries: 100\n",
      "Number of unsuccessful queries: 24\n"
     ]
    }
   ],
   "source": [
    "# 9a. Use ground truth to run test with smaller LLM\n",
    "\n",
    "MODEL_ID = \"mistral.mixtral-8x7b-instruct-v0:1\" # \"anthropic.claude-3-haiku-20240307-v1:0\" # \"mistral.mixtral-8x7b-instruct-v0:1\" \"anthropic.claude-3-5-sonnet-20240620-v1:0\" \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "\n",
    "# use helper class for threaded API calls\n",
    "wrapper = BedrockLLMWrapper(debug=False, model_id=MODEL_ID, max_token_count=500)\n",
    "util = Util()\n",
    "df1 = df_good_results\n",
    "prompts_list = []\n",
    "for row in df1.itertuples():\n",
    "    prompt = build_sqlquerygen_prompt(row.Question, row.Context)\n",
    "    prompts_list.append(prompt)\n",
    "results = wrapper.generate_threaded(prompts_list)\n",
    "\n",
    "# Create a list to store the generated SQL queries\n",
    "generated_sql_queries = []\n",
    "for result in results:\n",
    "    generated_sql_query = result[0].replace(\"\\\\\",\"\") # workaround, switching to ConverseAPI introduced \\ in Mistral response\n",
    "    generated_sql_queries.append(generated_sql_query)\n",
    "\n",
    "# Add the new column 'Generated_SQL_Query' to df_results\n",
    "df1['Generated_SQL_Query'] = generated_sql_queries\n",
    "\n",
    "\n",
    "# Test generated SQL queries and verify they work\n",
    "results = []\n",
    "\n",
    "for row in df1.itertuples():\n",
    "    statement = util.extract_with_regex(row.Generated_SQL_Query, util.SQL_PATTERN)\n",
    "    error = None\n",
    "    result = None\n",
    "    try:\n",
    "        \n",
    "        result = databaseutil.run_sql(statement)\n",
    "\n",
    "    except Exception as e:\n",
    "        error = e\n",
    "\n",
    "    results.append({'Question': row.Question,'Query': statement, 'Result': result, 'Error': error, 'ReferenceQuery': row.Query, 'Context': row.Context})\n",
    "\n",
    "\n",
    "# inspect first 3 results\n",
    "df1_results = pd.DataFrame(results)\n",
    "print(df1_results.head(3))\n",
    "\n",
    "# review successful/unsucessful queries\n",
    "df1_good_results = df1_results[df1_results['Error'].isnull() | (df1_results['Error'] == None)]\n",
    "print(f\"Number of successful queries: {len(df1_good_results)}\")\n",
    "\n",
    "df1_bad_results = df1_results[df1_results['Error'].notnull() | (df1_results['Error'] == 'None')]\n",
    "print(f\"Number of unsuccessful queries: {len(df1_bad_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Question  \\\n",
      "0         What is the total number of customers?   \n",
      "1  List all product names and their unit prices.   \n",
      "2    Who are the top 5 customers by order count?   \n",
      "\n",
      "                                               Query  \\\n",
      "0  SELECT COUNT(*) AS total_customers\\nFROM publi...   \n",
      "1  SELECT public.products.product_name, public.pr...   \n",
      "2  SELECT public.customers.customer_id, COUNT(pub...   \n",
      "\n",
      "                                              Result Error  \\\n",
      "0                                             [(91)]  None   \n",
      "1  [(Chai, 18.0), (Chang, 19.0), (Aniseed Syrup, ...  None   \n",
      "2  [(SAVEA, 31), (ERNSH, 30), (QUICK, 28), (HUNGO...  None   \n",
      "\n",
      "                                      ReferenceQuery  \\\n",
      "0                    SELECT COUNT(*) FROM customers;   \n",
      "1     SELECT product_name, unit_price FROM products;   \n",
      "2  SELECT c.company_name, COUNT(o.order_id) as or...   \n",
      "\n",
      "                                             Context  \n",
      "0  [{'table': 'customer_demographics', 'columns':...  \n",
      "1  [{'table': 'customer_demographics', 'columns':...  \n",
      "2  [{'table': 'customer_demographics', 'columns':...  \n",
      "Number of successful queries: 111\n",
      "Number of unsuccessful queries: 13\n"
     ]
    }
   ],
   "source": [
    "# 9b. Use ground truth to run test with larger LLM\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\" #\"anthropic.claude-3-sonnet-20240229-v1:0\"  \"mistral.mixtral-8x7b-instruct-v0:1\" \"anthropic.claude-3-5-sonnet-20240620-v1:0\" \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "\n",
    "# use helper class for threaded API calls\n",
    "wrapper = BedrockLLMWrapper(debug=False, model_id=MODEL_ID, max_token_count=500)\n",
    "util = Util()\n",
    "df2 = df_good_results\n",
    "prompts_list = []\n",
    "for row in df1.itertuples():\n",
    "    prompt = build_sqlquerygen_prompt(row.Question, row.Context)\n",
    "    prompts_list.append(prompt)\n",
    "results = wrapper.generate_threaded(prompts_list, max_workers=8)\n",
    "\n",
    "# Create a list to store the generated SQL queries\n",
    "generated_sql_queries = []\n",
    "for result in results:\n",
    "    generated_sql_query = result[0]\n",
    "    # print(f'generated_sql_query: {generated_sql_query}')\n",
    "    generated_sql_queries.append(generated_sql_query)\n",
    "\n",
    "# Add the new column 'Generated_SQL_Query' to df_results\n",
    "df2['Generated_SQL_Query'] = generated_sql_queries\n",
    "\n",
    "# Test generated SQL queries and verify they work\n",
    "results = []\n",
    "\n",
    "for row in df2.itertuples():\n",
    "    statement = util.extract_with_regex(row.Generated_SQL_Query, util.SQL_PATTERN)\n",
    "    # print(f'SQL statement: {statement}')\n",
    "    error = None\n",
    "    try:\n",
    "        \n",
    "        result = databaseutil.run_sql(statement)\n",
    "\n",
    "    except Exception as e:\n",
    "        error = e\n",
    "\n",
    "    results.append({'Question': row.Question,'Query': statement, 'Result': result, 'Error': error, 'ReferenceQuery': row.Query, 'Context': row.Context})\n",
    "\n",
    "df2_results = pd.DataFrame(results)\n",
    "\n",
    "# inspect first 3 results\n",
    "print(df2_results.head(3))\n",
    "\n",
    "# review successful/unsucessful queries\n",
    "df2_good_results = df2_results[df2_results['Error'].isnull() | (df2_results['Error'] == None)]\n",
    "print(f\"Number of successful queries: {len(df2_good_results)}\")\n",
    "\n",
    "df2_bad_results = df2_results[df2_results['Error'].notnull() | (df2_results['Error'] == 'None')]\n",
    "print(f\"Number of unsuccessful queries: {len(df2_bad_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As expected, we can observe that a larger LLM (e.g. Haiku) is able to produce valid SQL queries slightly more successfully with zero-shot prompting compared to a smaller LLM (e.g. Mistral 8x7b)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-router-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
